---
layout: post

title: 06-卷积神经网络

date: 2025-02-28 13:30:23 +0900

categories: [深度学习]
---

- [6.1  从全连接层到卷积](#61--从全连接层到卷积)
- [6.2  图像卷积](#62--图像卷积)
  - [互相关运算](#互相关运算)
  - [卷积层](#卷积层)
  - [图像中目标的边缘检测](#图像中目标的边缘检测)
  - [学习卷积核](#学习卷积核)
- [6.3  填充和步幅](#63--填充和步幅)
  - [填充](#填充)
  - [步幅](#步幅)
- [6.4  多输入多输出通道](#64--多输入多输出通道)
  - [多输入通道](#多输入通道)
  - [多输出通道](#多输出通道)
  - [1x1卷积层](#1x1卷积层)
- [6.5  汇聚层](#65--汇聚层)
  - [最大汇聚层和平均汇聚层](#最大汇聚层和平均汇聚层)
- [6.6  卷积神经网络](#6--卷积神经网络)
  - [LeNet](#lenet)
  - [模型训练](#模型训练)



### 6.1  从全连接层到卷积

### 6.2  图像卷积

#### 互相关运算

在卷积层中，输入张量和核张量通过*互相关运算*产生输出张量

**二维互相关运算：**

<p>
    <img src="https://hhhi21g.github.io/assets/img/deepLearning/chapter6/cha60.png" alt="alt text" style="zoom:67%;" />
</p>


- 输出大小：(n<sub>h</sub> - k<sub>h</sub> + 1) x (n<sub>w</sub> - k<sub>w</sub> + 1)

- 保存至d2l:

  ```python
  # 进行互相关运算
  def corr2d(X, K):  # @save
      h, w = K.shape
      Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))
      for i in range(Y.shape[0]):
          for j in range(Y.shape[1]):
              Y[i,j] = (X[i:i + h, j:j + w] * K).sum()
      return Y
  ```

  ```python
  X = torch.tensor([[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]])
  K = torch.tensor([[0.0, 1.0], [2.0, 3.0]])
  print(corr2d(X, K))
  ```

  > tensor([[19., 25.],
  >         [37., 43.]])

------

#### 卷积层

初始化权重和偏置：

```python
class Conv2D(nn.Module):
    def __init__(self, kernel_size):
        super().__init__()
        self.weight = nn.Parameter(torch.rand(kernel_size))
        self.bias = nn.Parameter(torch.zeros(1))

    def forward(self, x):
        return corr2d(x, self.weight) + self.bias
```

------

#### 图像中目标的边缘检测

构造一个黑白图像，其中黑色(0)，白色(1)：

```python
X = torch.ones((6, 8))
X[:, 2:6] = 0
print(X)
```

> tensor([[1., 1., 0., 0., 0., 0., 1., 1.],<br/>
>         [1., 1., 0., 0., 0., 0., 1., 1.],<br/>
>         [1., 1., 0., 0., 0., 0., 1., 1.],<br/>
>         [1., 1., 0., 0., 0., 0., 1., 1.],<br/>
>         [1., 1., 0., 0., 0., 0., 1., 1.],<br/>
>         [1., 1., 0., 0., 0., 0., 1., 1.]])

构造一个高度为1、宽度为2的卷积核K：`K = torch.tensor([[1.0, -1.0]])`

进行互相关运算，如果水平相邻两元素相同，则输出为0，否则输出为非0

```python
Y = corr2d(X, K)
print(Y)
```

> tensor([[ 0.,  1.,  0.,  0.,  0., -1.,  0.],<br/>
>         [ 0.,  1.,  0.,  0.,  0., -1.,  0.],<br/>
>         [ 0.,  1.,  0.,  0.,  0., -1.,  0.],<br/>
>         [ 0.,  1.,  0.,  0.,  0., -1.,  0.],<br/>
>         [ 0.,  1.,  0.,  0.,  0., -1.,  0.],<br/>
>         [ 0.,  1.,  0.,  0.,  0., -1.,  0.]])

1:从白色到黑色的边缘；&nbsp; 0:从黑色到白色的边缘；

------

#### 学习卷积核

学习生成卷积核：

- 先构造一个卷积层，并将其卷积核初始化为随机张量；

- 接下来在每次迭代中，比较Y与卷积层输出的平方误差，计算梯度来更新卷积核

```python
conv2d = nn.Conv2d(1, 1, kernel_size=(1, 2), bias=False)

# 使用四维输入和输出格式(批量大小、通道、高度、宽度)
X = X.reshape((1, 1, 6, 8))
Y = Y.reshape((1, 1, 6, 7))
lr = 3e-2

for i in range(10):
    Y_hat = conv2d(X)
    l = (Y_hat - Y) ** 2
    conv2d.zero_grad()
    l.sum().backward()
    # 迭代卷积核
    conv2d.weight.data[:] -= lr * conv2d.weight.grad
    if (i + 1) % 2 == 0:
        print(f'epoch {i + 1},loss {l.sum():.3f}')
```

> epoch 2,loss 1.184<br/>
> epoch 4,loss 0.199<br/>
> epoch 6,loss 0.034<br/>
> epoch 8,loss 0.006<br/>
> epoch 10,loss 0.001

```python
print(conv2d.weight.data.reshape((1, 2)))
```

> tensor([[ 0.9935, -0.9952]])

经学习后，卷积核权重非常接近之前定义的卷积核

------

### 6.3  填充和步幅

#### 填充

在应用多层卷积时，我们常常丢失边缘像素；解决这个问题的简单方法即为*填充*：

在输入图像的边界填充元素，通常为0

<p>
    <img src="https://hhhi21g.github.io/assets/img/deepLearning/chapter6/cha61.png" alt="alt text" style="zoom:67%;" />
</p>


输出形状: (n<sub>h</sub> - k<sub>h</sub> + p<sub>h</sub> + 1) x (n<sub>w</sub> - k<sub>w</sub> + p<sub>w</sub> + 1)

**卷积核的高度和宽度通常为奇数：**

保持空间维度的同时，可以在顶部和底部填充相同数量的行，在左侧和右侧填充相同数量的列；

对于任何二维张量X，当满足：

- 卷积核的大小是奇数；
- 所有边的填充行数和列数相同；
- 输出和输入具有相同高度和宽度;

则有：输出Y[i,j]是通过以输入X[i,j]为中心，与卷积核进行互相关计算得到的。

```python
def comp_conv2d(conv2d,X):
    # (1，1)表示批量大小，通道数
    X = X.reshape((1,1) + X.shape)
    Y = conv2d(X)
    # 返回高和宽
    return Y.reshape(Y.shape[2:])
```

```python
# padding:表示每边都填充了1行或1列.共添加2
conv2d = nn.Conv2d(1,1,kernel_size=3,padding=1)
X = torch.rand(size=(8,8))
print(comp_conv2d(conv2d,X).shape)

conv2d = nn.Conv2d(1,1,kernel_size=(5,3),padding=(2,1))
print(comp_conv2d(conv2d,X).shape)
```

> torch.Size([8, 8])<br/>
> torch.Size([8, 8])

------

#### 步幅

默认每次滑动一个元素，但有时候为了高效计算或是缩减采样次数，卷积窗口可以跳过中间位置，每次滑动多个元素。

<p>
    <img src="https://hhhi21g.github.io/assets/img/deepLearning/chapter6/cha62.png" alt="alt text" style="zoom:67%;" />
</p>

<p>
    <img src="https://hhhi21g.github.io/assets/img/deepLearning/chapter6/cha63.png" alt="alt text" style="zoom:67%;" />
</p>


```python
conv2d = nn.Conv2d(1,1,kernel_size=3,padding=1,stride=2)
print(comp_conv2d(conv2d,X).shape)

conv2d = nn.Conv2d(1,1,kernel_size=(3,5),padding=(0,1),stride=(3,4))
print(comp_conv2d(conv2d,X).shape)
```

> torch.Size([4, 4])<br/>
> torch.Size([2, 2])

在实践中，很少使用不一致的步幅或填充。

------

### 6.4  多输入多输出通道

#### 多输入通道

当输入包含多个通道时，需要构造一个*与输入数据具有相同输入通道数*的卷积核

<p>
    <img src="https://hhhi21g.github.io/assets/img/deepLearning/chapter6/cha64.png" alt="alt text" style="zoom:67%;" />
</p>


```python
def corr2d_multi_in(X, K):
    return sum(d2l.corr2d(x, k) for x, k in zip(X, K))
    # zip():将X,K一一配对
```

```python
X = torch.tensor([[[0, 1, 2], [3, 4, 5], [6, 7, 8]],
                  [[1, 2, 3], [4, 5, 6], [7, 8, 9]]])
K = torch.tensor([[[0, 1], [2, 3]], [[1, 2], [3, 4]]])

print(corr2d_multi_in(X, K))
```

------

#### 多输出通道

在最流行的神经网络架构中，随着神经网络层数的加深，我们常会增加输出通道的维数，通过减少空间分辨率以获得更大的通道深度。

```python
def corr2d_multi_in_out(X, K):
    # 沿维度0堆叠结果
    return torch.stack([corr2d_multi_in(X, k) for k in K], 0)
```

```python
K = torch.stack((K, K + 1, K + 2), 0)
print(K.shape)

print(corr2d_multi_in_out(X, K))
```

> torch.Size([3, 2, 2, 2])<br/>
> tensor([[[ 56.,  72.],
>                [104., 120.]],<br/>
>
> ​              [[ 76., 100.],
>
> ​               [148., 172.]],<br/>
>
> ​              [[ 96., 128.],
> ​               [192., 224.]]])

------

#### 1x1卷积层

<p>
    <img src="https://hhhi21g.github.io/assets/img/deepLearning/chapter6/cha65.png" alt="alt text" style="zoom:67%;" />
</p>


```python
def corr2d_multi_in_out_1x1(X, K):
    c_i, h, w = X.shape
    c_o = K.shape[0]
    X = X.reshape((c_i, h * w))
    K = K.reshape((c_o, c_i))

    Y = torch.matmul(X, K)
    return Y.reshape((c_o, h, w))
```

当执行1×1卷积运算时，上述函数相当于先前实现的互相关函数`corr2d_multi_in_out`

```python
X = torch.normal(0, 1, (3, 3, 3))
K = torch.normal(0, 1, (2, 3, 1, 1,))

Y1 = corr2d_multi_in_out_1x1(X, K)
Y2 = corr2d_multi_in_out_1x1(X, K)
assert float(torch.abs(Y1 - Y2).sum()) < 1e-6
```

------

### 6.5  汇聚层

#### 最大汇聚层和平均汇聚层

池运算是确定性的，我们通常计算汇聚窗口中所有元素的最大值或平均值，这些操作分别称为*最大汇聚层*和*平均汇聚层*。

<p>
    <img src="https://hhhi21g.github.io/assets/img/deepLearning/chapter6/cha66.png" alt="alt text" style="zoom:67%;" />
</p>


```python
def pool2d(X, pool_size, mode='max'):
    p_h, p_w = pool_size
    Y = torch.zeros((X.shape[0] - p_h + 1, X.shape[1] - p_w + 1))
    for i in range(Y.shape[0]):
        for j in range(Y.shape[1]):
            if mode == 'max':
                Y[i, j] = X[i:i + p_h, j:j + p_w].max()
            elif mode == 'avg':
                Y[i, j] = X[i:i + p_h, j:j + p_w].mean()
    return Y
```

```python
X = torch.tensor([[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]])
print(pool2d(X, (2, 2)))
print(pool2d(X,(2,2),'avg'))
```

> tensor([[4., 5.],
>         [7., 8.]])<br/>
> tensor([[2., 3.],
>         [5., 6.]])

**汇聚层中的填充和步幅**

默认情况下，深度学习框架中的步幅与汇聚窗口的大小相同

```python
X = torch.arange(16, dtype=torch.float32).reshape((1, 1, 4, 4))

# 设置汇聚窗口为(3,3)
pool2d = nn.MaxPool2d(3)
print(pool2d(X))
```

> tensor([[[[10.]]]])

```python
pool2d = nn.MaxPool2d(3,padding=1,stride=2)
print(pool2d(X))
```

> tensor([[[[ 5.,  7.],<br/>
>           [13., 15.]]]])

**汇聚层中的多个通道**

```python
X = torch.cat((X, X + 1), 1)
print(X.shape)

pool2d = nn.MaxPool2d(3, padding=1, stride=2)
print(pool2d(X))
```

> torch.Size([1, 2, 4, 4])<br/>
> tensor([[[[ 5.,  7.],
>           [13., 15.]],<br/>
>
> ​          [[ 6.,  8.],
> ​           [14., 16.]]]])

------

### 卷积神经网络(LeNet)

#### LeNet

<p>
    <img src="https://hhhi21g.github.io/assets/img/deepLearning/chapter6/cha67.png" alt="alt text" style="zoom:67%;" />
</p>
<p>
        <img src="https://hhhi21g.github.io/assets/img/deepLearning/chapter6/cha68.png" alt="alt text" style="zoom:67%;" />
</p>


```python
net = nn.Sequential(
    nn.Conv2d(1,6,kernel_size=5,padding=2),nn.Sigmoid(),
    nn.AvgPool2d(kernel_size=2,stride=2),
    nn.Conv2d(6,16,kernel_size=5),nn.Sigmoid(),
    nn.AvgPool2d(kernel_size=2,stride=2),
    nn.Flatten(),
    nn.Linear(16*5*5,120),nn.Sigmoid(),
    nn.Linear(120,84),nn.Sigmoid(),
    nn.Linear(84,10)
)

X = torch.rand(size=(1,1,28,28),dtype=torch.float32)
for layer in net:
    X = layer(X)
    print(layer.__class__.__name__,'output shape:\t',X.shape)
```

> Conv2d output shape:	 torch.Size([1, 6, 28, 28])<br/>
> Sigmoid output shape:	 torch.Size([1, 6, 28, 28])<br/>
> AvgPool2d output shape:	 torch.Size([1, 6, 14, 14])<br/>
> Conv2d output shape:	 torch.Size([1, 16, 10, 10])<br/>
> Sigmoid output shape:	 torch.Size([1, 16, 10, 10])<br/>
> AvgPool2d output shape:	 torch.Size([1, 16, 5, 5])<br/>
> Flatten output shape:	 torch.Size([1, 400])<br/>
> Linear output shape:	 torch.Size([1, 120])<br/>
> Sigmoid output shape:	 torch.Size([1, 120])<br/>
> Linear output shape:	 torch.Size([1, 84])<br/>
> Sigmoid output shape:	 torch.Size([1, 84])<br/>
> Linear output shape:	 torch.Size([1, 10])

------

#### 模型训练

```python
batch_size = 256
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size=batch_size)
```

```python
lr, num_epochs = 0.9, 10
train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())
plt.show()
```

**保存至d2l:**

```python
# @save
def train_ch6(net, train_iter, test_iter, num_epochs, lr, device):
    """用GPU训练模型(在第六章定义)"""

    def init_weights(m):
        if type(m) == nn.Linear or type(m) == nn.Conv2d:
            nn.init.xavier_uniform_(m.weight)

    net.apply(init_weights)
    print('training on', device)
    net.to(device)
    optimizer = torch.optim.SGD(net.parameters(), lr=lr)
    loss = nn.CrossEntropyLoss()
    animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs],
                            legend=['train loss', 'train acc', 'test acc'])
    timer, num_batches = d2l.Timer(), len(train_iter)
    for epoch in range(num_epochs):
        # 训练损失之和，训练准确率之和，样本数
        metric = d2l.Accumulator(3)
        net.train()
        for i, (X, y) in enumerate(train_iter):
            timer.start()
            optimizer.zero_grad()
            X, y = X.to(device), y.to(device)
            y_hat = net(X)
            l = loss(y_hat, y)
            l.backward()
            optimizer.step()
            with torch.no_grad():
                metric.add(l * X.shape[0], d2l.accuracy(y_hat, y), X.shape[0])
            timer.stop()
            train_l = metric[0] / metric[2]
            train_acc = metric[1] / metric[2]
            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:
                animator.add(epoch + (i + 1) / num_batches,
                             (train_l, train_acc, None))
        test_acc = evaluate_accuracy_gpu(net, test_iter)
        animator.add(epoch + 1, (None, None, test_acc))
    print(f'loss {train_l:.3f}, train acc {train_acc:.3f}, '
          f'test acc {test_acc:.3f}')
    print(f'{metric[2] * num_epochs / timer.sum():.1f} examples/sec '
          f'on {str(device)}')
```

```python
def evaluate_accuracy_gpu(net, data_iter, device=None):  # @save
    """使用GPU计算模型在数据集上的精度"""
    if isinstance(net, nn.Module):
        net.eval()  # 设置为评估模式
        if not device:
            device = next(iter(net.parameters())).device
    # 正确预测的数量，总预测的数量
    metric = d2l.Accumulator(2)
    with torch.no_grad():
        for X, y in data_iter:
            if isinstance(X, list):
                # BERT微调所需的（之后将介绍）
                X = [x.to(device) for x in X]
            else:
                X = X.to(device)
            y = y.to(device)
            metric.add(d2l.accuracy(net(X), y), y.numel())
    return metric[0] / metric[1]
```

> loss 0.465, train acc 0.827, test acc 0.810<br/>
> 58022.3 examples/sec on cuda:0

<p>
    <img src="https://hhhi21g.github.io/assets/img/deepLearning/chapter6/cha69.png" alt="alt text" style="zoom:67%;" />
</p>

------

