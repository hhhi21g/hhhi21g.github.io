---
layout: post

title: 03-çº¿æ€§ç¥ç»ç½‘ç»œ

date: 2025-01-25 00:08:23 +0900

categories: [DeepLearning]
---

- [3. çº¿æ€§ç¥ç»ç½‘ç»œ](#3-çº¿æ€§ç¥ç»ç½‘ç»œ)
  
  - [3.1  çº¿æ€§å›å½’ğŸ˜‚](#31--çº¿æ€§å›å½’)
    
    - [çº¿æ€§æ¨¡å‹](#çº¿æ€§æ¨¡å‹)
    - [æŸå¤±å‡½æ•°](#æŸå¤±å‡½æ•°)
    - [éšæœºæ¢¯åº¦ä¸‹é™](#éšæœºæ¢¯åº¦ä¸‹é™)
    - [çŸ¢é‡åŒ–åŠ é€Ÿ](#çŸ¢é‡åŒ–åŠ é€Ÿ)
    - [æ­£æ€åˆ†å¸ƒç»˜å›¾å®ä¾‹](#æ­£æ€åˆ†å¸ƒç»˜å›¾å®ä¾‹)
    - [çº¿æ€§ç¥ç»å›¾](#çº¿æ€§ç¥ç»å›¾)
    
  - [3.2  çº¿æ€§å›å½’çš„ä»é›¶å¼€å§‹å®ç°](#32--çº¿æ€§å›å½’çš„ä»é›¶å¼€å§‹å®ç°)
  
  - [3.3  çº¿æ€§å›å½’çš„ç®€æ´å®ç°](#33--çº¿æ€§å›å½’çš„ç®€æ´å®ç°)
    - [ç”Ÿæˆæ•°æ®é›†](#ç”Ÿæˆæ•°æ®é›†)
    - [è¯»å–æ•°æ®é›†](#è¯»å–æ•°æ®é›†)
    - [å®šä¹‰æ¨¡å‹](#å®šä¹‰æ¨¡å‹)
    - [åˆå§‹åŒ–æ¨¡å‹å‚æ•°](#åˆå§‹åŒ–æ¨¡å‹å‚æ•°)
    - [å®šä¹‰æŸå¤±å‚æ•°](#å®šä¹‰æŸå¤±å‚æ•°)
    - [å®šä¹‰ä¼˜åŒ–ç®—æ³•](#å®šä¹‰ä¼˜åŒ–ç®—æ³•)
    - [è®­ç»ƒ](#è®­ç»ƒ)
    
  - [3.4  softmaxå›å½’](#34--softmaxå›å½’)
    - [softmaxç®€è¦è®¾è®¡åŸç†](#softmaxç®€è¦è®¾è®¡åŸç†)
    - [æŸå¤±å‡½æ•°](#æŸå¤±å‡½æ•°)
    
  - [3.5  å›¾åƒåˆ†ç±»æ•°æ®é›†](#35--å›¾åƒåˆ†ç±»æ•°æ®é›†)
    - [ç»„ä»¶ä»¬](#ç»„ä»¶ä»¬)
    - [æ•´åˆç»„ä»¶](#æ•´åˆç»„ä»¶)
    
  - [3.6  softmaxå›å½’çš„ä»é›¶å¼€å§‹å®ç°](#36--softmaxå›å½’çš„ä»é›¶å¼€å§‹å®ç°)
    - [ä¿å­˜è‡³d2l](#ä¿å­˜è‡³d2l)
    - [ä½¿ç”¨å®ä¾‹](#ä½¿ç”¨å®ä¾‹)
    
  - [3.7  softmaxå›å½’çš„ç®€æ´å®ç°](#37--softmaxå›å½’çš„ç®€æ´å®ç°)
    - [æŸå¤±å‡½æ•°çš„æ•°å­¦åŸç†](#æŸå¤±å‡½æ•°çš„æ•°å­¦åŸç†)
    
    - [å®ç°](#å®ç°)
    
      

### 3.  çº¿æ€§ç¥ç»ç½‘ç»œ

#### 3.1  çº¿æ€§å›å½’ğŸ˜‚

##### çº¿æ€§æ¨¡å‹

å‘é‡**x**: æ‰€æœ‰ç‰¹å¾

å‘é‡**w**: æ‰€æœ‰æƒé‡  (å‘é‡ä¸ºå‚ç›´æ–¹å‘æ’åˆ—,  ç‚¹ç§¯)
<p align="center">
    <img src="https://hhhi21g.github.io/public/img/pic306.png" alt="alt text" style="zoom:50%;" />
</p>


çŸ©é˜µ**X**: æ¯ä¸€è¡Œæ˜¯ä¸€ä¸ªæ ·æœ¬ï¼Œæ¯ä¸€åˆ—æ˜¯ä¸€ç§ç‰¹å¾ï¼ˆçŸ©é˜µ-å‘é‡ä¹˜æ³•ï¼‰
<p align="center">
    <img src="https://hhhi21g.github.io/public/img/pic305.png" alt="alt text" style="zoom:50%;" />
</p>


------

##### æŸå¤±å‡½æ•°

å¹³æ–¹è¯¯å·®(0.5ä½¿å¾—æ±‚å¯¼åå¸¸æ•°ç³»æ•°ä¸º1)ï¼š

<p align="center">
    <img src="https://hhhi21g.github.io/public/img/pic303.png" alt="alt text" style="zoom:50%;" />
</p>


æŸå¤±å‡å€¼ï¼š

<p align="center">
    <img src="https://hhhi21g.github.io/public/img/pic304.png" alt="alt text" style="zoom:50%;" />
</p>


------

##### éšæœºæ¢¯åº¦ä¸‹é™

**ÃŸ**: éšæœºå–æ ·çš„ä¸€ä¸ªå°æ‰¹é‡ï¼Œç”±å›ºå®šæ•°é‡çš„è®­ç»ƒæ ·æœ¬ç»„æˆï¼ˆ|ÃŸ|ï¼šæ‰¹é‡å¤§å°)

**Å‹**:  å­¦ä¹ ç‡

é€šå¸¸æ‰‹åŠ¨é¢„å…ˆæŒ‡å®šï¼Œè¿™äº›å¯ä»¥è°ƒæ•´ä½†ä¸åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ›´æ–°çš„å‚æ•°ç§°ä¸º***è¶…å‚æ•°*** ï¼Œ***è°ƒå‚***æ˜¯é€‰æ‹©è¶…å‚æ•°çš„è¿‡ç¨‹ï¼Œè®­ç»ƒè¿­ä»£ç»“æœæ˜¯åœ¨ç‹¬ç«‹çš„***éªŒè¯æ•°æ®é›†***ä¸Šè¯„ä¼°å¾—åˆ°çš„ã€‚

<p align="center">
    <img src="https://hhhi21g.github.io/public/img/pic301.png" alt="alt text" style="zoom:67%;" />
</p>


å…¬å¼åé¢ä¸ºè¯¯å·®å‡å€¼ï¼ŒåŸå€¼ - å­¦ä¹ ç‡ * è¯¯å·®å‡å€¼ ä»¥ä¿®æ­£

***æ³›åŒ–***ï¼šæ‰¾åˆ°ä¸€ç»„å‚æ•°ï¼Œèƒ½å¤Ÿåœ¨æˆ‘ä»¬ä»æœªè§è¿‡çš„æ•°æ®ä¸Šå®ç°è¾ƒä½çš„æŸå¤±

------

##### çŸ¢é‡åŒ–åŠ é€Ÿ

#@saveç±»Timer

`start:  # å¯åŠ¨è®¡æ—¶å™¨`

`stop:  # åœæ­¢è®¡æ—¶å™¨å¹¶å°†æ—¶é—´è®°å½•åœ¨åˆ—è¡¨ä¸­`

`avg:  # è¿”å›å¹³å‡æ—¶é—´`

`sum:  # è¿”å›æ—¶é—´æ€»å’Œ`

`cumsum:  # è¿”å›ç´¯è®¡æ—¶é—´`

ä½¿ç”¨å®ä¾‹ï¼š

```python
n = 10000
a = torch.ones([n])
b = torch.ones([n])

timer = Timer()
d = a + b
print(f'{timer.stop():.5f} sec')  # çŸ¢é‡åŒ–ä»£ç é€šå¸¸ä¼šå¸¦æ¥æ•°é‡çº§çš„åŠ é€Ÿ
```

------

##### æ­£æ€åˆ†å¸ƒç»˜å›¾å®ä¾‹

æ¦‚ç‡å¯†åº¦å‡½æ•°ï¼š

```python
def normal(x, mu, sigma):
    p = 1 / math.sqrt(2 * math.pi * sigma ** 2)
    return p * np.exp(-(x - mu) ** 2 / (2 * sigma ** 2))
```

å¯è§†åŒ–ï¼š

```python
x = np.arange(-7, 7, 0.01)

params = [(0, 1), (0, 2), (3, 1)]
d2l.plot(x, [normal(x, mu, sigma) for mu, sigma in params], xlabel='x',
         ylabel='p(x)', figsize=(4.5, 2.5),
         legend=[f'mean {mu},std {sigma}' for mu, sigma in params])
plt.show()
```

<p align="center">
    <img src="https://hhhi21g.github.io/public/img/pic302.png" alt="alt text" style="zoom:67%;" />
</p>


------

##### çº¿æ€§ç¥ç»å›¾

**ç¥ç»ç½‘ç»œçš„å±‚æ•°**ï¼šè®¡ç®—å±‚æ•°æ—¶ä¸è€ƒè™‘è¾“å…¥å±‚

**å…¨è¿æ¥å±‚ï¼ˆç¨ å¯†å±‚ï¼‰**ï¼šæ¯ä¸ªè¾“å…¥éƒ½ä¸æ¯ä¸ªè¾“å‡ºç›¸è¿

------

<br/>

<br/>

#### 3.2  çº¿æ€§å›å½’çš„ä»é›¶å¼€å§‹å®ç°

**ä¿å­˜è‡³d2lçš„å‡½æ•°**ï¼š

```python
# ç”Ÿæˆy = Xw + b + å™ªå£°
def synthetic_data(w, b, num_examples):  # @save
    X = torch.normal(0, 1, (num_examples, len(w)))
    y = torch.matmul(X, w) + b
    y += torch.normal(0, 0.01, y.shape)
    return X, y.reshape((-1, 1))
```

```python
# çº¿æ€§å›å½’æ¨¡å‹
def linreg(X, w, b):  # @save
    return torch.matmul(X, w) + b
```

```python
# æŸå¤±å‡½æ•°
def squared_loss(y_hat, y):  # @save
    return (y_hat - y.reshape(y_hat.shape)) ** 2 / 2
```

```python
# å°æ‰¹é‡éšæœºæ¢¯åº¦ä¸‹é™
def sgd(params, lr, batch_size):  # @save
    with torch.no_grad():  # ä¸è¿›è¡Œæ¢¯åº¦è¿½è¸ªï¼ŒèŠ‚çœå†…å­˜å’Œè®¡ç®—èµ„æº
        for param in params:
            param -= lr * param.grad / batch_size
            param.grad.zero_()
```

**ç»˜åˆ¶æ•£ç‚¹å›¾å®ä¾‹**ï¼š

```python
d2l.set_figsize()
d2l.plt.scatter(features[:, (1)].detach().numpy(), labels.detach().numpy(), 1)
plt.show()
```

**éšæœºè¯»å–å°æ‰¹é‡æ ·æœ¬å®ä¾‹**ï¼š

```python
def data_iter(batch_size, features, labels):
    num_examples = len(features)
    indices = list(range(num_examples))
    random.shuffle(indices)
    for i in range(0, num_examples, batch_size):
        batch_indices = torch.tensor(
            indices[i:min(i + batch_size, num_examples)]
        )
        yield features[batch_indices], labels[batch_indices]
```

**è®­ç»ƒå®ä¾‹**ï¼š

```python
lr = 0.03
num_epochs = 3
net = linreg
loss = squared_loss

for epoch in range(num_epochs):
    for X, y in data_iter(batch_size, features, labels):
        l = loss(net(X, w, b), y)  # lçš„å½¢çŠ¶ä¸º[batch_size,1],éæ ‡é‡
        l.sum().backward()
        sgd([w, b], lr, batch_size) 
    with torch.no_grad():
        train_l = loss(net(features, w, b), labels)
        print(f'epoch {epoch + 1}, loss {float(train_l.mean()):f}')
```

------

<br/>

<br/>

#### 3.3  çº¿æ€§å›å½’çš„ç®€æ´å®ç°

import:

```python
import torch
from d2l import torch as d2l
from torch.utils import data
```

**ä¿å­˜è‡³d2lçš„å‡½æ•°**ï¼š

```python
# æ„é€ ä¸€ä¸ªpytorchæ•°æ®è¿­ä»£å™¨
def load_array(data_arrays, batch_size, is_train=True):  # @save
    dataset = data.TensorDataset(*data_arrays)  # å°†è¾“å…¥çš„å¼ é‡åŒ…è£…æˆä¸€ä¸ªæ•°æ®é›†å¯¹è±¡
    return data.DataLoader(dataset, batch_size, shuffle=is_train)
```

1. ##### ç”Ÿæˆæ•°æ®é›†

   ```python
   true_w = torch.tensor([2, -3.4])
   true_b = 4.2
   features, labels = d2l.synthetic_data(true_w, true_b, 1000)
   ```

2. ##### è¯»å–æ•°æ®é›†

   ```python
   batch_size = 10
   data_iter = load_array((features, labels), batch_size)
   ```

   ```python
   print(next(iter(data_iter)))  # iter()æ˜¾å¼åœ°åˆ›å»ºä¸€ä¸ªè¿­ä»£å™¨
   # iter()å‡½æ•°è¿”å›çš„æ˜¯ä¸€ä¸ªè¿­ä»£å™¨ï¼Œæœ¬èº«å¹¶ä¸è¿”å›æ•°æ®;
   # åªæœ‰è°ƒç”¨next()æ—¶ï¼Œè¿­ä»£å™¨æ‰ä¼šå¼€å§‹è¿­ä»£å¹¶è¿”å›æ•°æ®
   ```

   > [tensor([[-0.4443, -1.2958],
   >         [-0.7646,  0.6442],
   >         [ 0.0836, -1.3110],
   >         [-0.4851, -1.1598],
   >         [ 0.4348, -0.1553],
   >         [ 0.2450, -0.4867],
   >         [ 0.2895,  1.1791],
   >         [-0.3936,  0.7068],
   >         [ 0.6898,  0.2431],
   >         [ 1.4839, -0.4528]]),<br/> tensor([[7.7131],
   >         [0.4788],
   >         [8.8145],
   >         [7.1686],
   >         [5.6042],
   >         [6.3388],
   >         [0.7815],
   >         [1.0104],
   >         [4.7435],
   >         [8.7051]])]


##### å®šä¹‰æ¨¡å‹

```python
from torch import nn
net = nn.Sequential(nn.Linear(2, 1))
```

1. ##### åˆå§‹åŒ–æ¨¡å‹å‚æ•°

   ```python
   net[0].weight.data.normal_(0, 0.01)
   net[0].bias.data.fill_(0)
   ```

2. ##### å®šä¹‰æŸå¤±å‚æ•°

   ```python
   # MSELossç±»ï¼Œä¹Ÿç§°å¹³æ–¹L2èŒƒæ•°ï¼Œ é»˜è®¤æƒ…å†µè¿”å›æ‰€æœ‰æ ·æœ¬æŸå¤±çš„å¹³å‡å€¼
   loss = nn.MSELoss()
   ```

3. ##### å®šä¹‰ä¼˜åŒ–ç®—æ³•

   ```python
   trainer = torch.optim.SGD(net.parameters(), lr=0.03)
   ```

4. ##### è®­ç»ƒ

   ```python
   num_epochs = 3
   for epoch in range(num_epochs):
       for X, y in data_iter:
           l = loss(net(X), y)
           trainer.zero_grad()
           l.backward()
           trainer.step()
           # æ ¹æ®å·²ç»è®¡ç®—çš„æ¢¯åº¦æ›´æ–°æ¨¡å‹çš„å‚æ•°
           # æ ¹æ®æŒ‡å®šçš„ä¼˜åŒ–ç®—æ³•(SGD,Adamç­‰)è°ƒæ•´å‚æ•°
       l = loss(net(features), labels)
       print(f'epoch {epoch + 1}, loss {l:f}')
   ```

   > epoch 1, loss 0.000246<br/>
   > epoch 2, loss 0.000102<br/>
   > epoch 3, loss 0.000101

   ```python
   # æ¯”è¾ƒç”Ÿæˆæ•°æ®é›†çš„çœŸå®å‚æ•°å’Œé€šè¿‡æœ‰é™æ•°æ®è®­ç»ƒè·å¾—çš„æ¨¡å‹å‚æ•°
   w = net[0].weight.data
   print('wçš„ä¼°è®¡è¯¯å·®:',true_w - w.reshape(true_w.shape))
   b = net[0].bias.data
   print('bçš„ä¼°è®¡è¯¯å·®:',true_b - b)
   ```

   > wçš„ä¼°è®¡è¯¯å·®: tensor([-2.2888e-05,  3.2783e-04])<br/>
   > bçš„ä¼°è®¡è¯¯å·®: tensor([0.0002])

------

<br/>

<br/>

#### 3.4  softmaxå›å½’

***å•å±‚ç¥ç»ç½‘ç»œï¼Œå…¨è¿æ¥å±‚***

è¾“å‡ºç”±æƒé‡å’Œè¾“å…¥ç‰¹å¾è¿›è¡Œ*çŸ©é˜µ-å‘é‡*ä¹˜æ³•åŠ ä¸Šåç½®å¾—åˆ°:  **o** = **Wx** + **b**

------

##### softmaxç®€è¦è®¾è®¡åŸç†

æ¨¡å‹è¾“å‡ºÅ·<sub>j</sub>: å¯ä»¥è§†ä¸ºå±äºç±»*j*çš„æ¦‚ç‡  --->  é€‰æ‹©å…·æœ‰æœ€å¤§è¾“å‡ºå€¼çš„ç±»åˆ«ä½œä¸ºé¢„æµ‹

ä½†ç›´æ¥å°†æœªè§„èŒƒåŒ–çš„é¢„æµ‹oè§†ä¸ºè¾“å‡º**å­˜åœ¨é—®é¢˜**ï¼š

- æ²¡æœ‰é™åˆ¶è¾“å‡ºæ•°å­—æ€»å’Œä¸º1ï¼Œå³æ¦‚ç‡å’Œæœªå¿…ä¸º1
- æ ¹æ®è¾“å…¥çš„ä¸åŒï¼Œè¾“å‡ºå¯èƒ½ä¸ºè´Ÿå€¼

softmaxå‡½æ•°èƒ½å¤Ÿå°†æœªè§„èŒƒåŒ–çš„é¢„æµ‹å˜æ¢ä¸ºéè´Ÿæ•°å¹¶ä¸”æ€»å’Œä¸º1ï¼ŒåŒæ—¶è®©æ¨¡å‹ä¿æŒå¯å¯¼æ€§è´¨

- æ±‚å¹‚ï¼Œç¡®ä¿è¾“å‡ºéè´Ÿ
- æ¯ä¸ªæ±‚å¹‚çš„ç»“æœé™¤ä»¥å®ƒä»¬çš„æ€»å’Œï¼Œæœ€ç»ˆè¾“å‡ºçš„æ¦‚ç‡å€¼æ€»å’Œä¸º1

<p align="center">
    <img src="https://hhhi21g.github.io/public/img/pic340.svg" alt="alt text" style="zoom:100%;" />
</p>



å°½ç®¡softmaxæ˜¯ä¸€ä¸ªéçº¿æ€§å‡½æ•°ï¼Œä½†softmaxå›å½’çš„è¾“å‡ºä»ç„¶ç”±è¾“å…¥ç‰¹å¾çš„ä»¿å°„å˜æ¢å†³å®šï¼Œå› æ­¤softmaxå›å½’æ˜¯ä¸€ä¸ª***çº¿æ€§æ¨¡å‹***ã€‚

------

##### æŸå¤±å‡½æ•°

ä½¿ç”¨æœ€å¤§ä¼¼ç„¶ä¼°è®¡ï¼šé€šå¸¸è¢«ç§°ä¸º***äº¤å‰ç†µæŸå¤±***

<p align="center">
    <img src="https://hhhi21g.github.io/public/img/pic341.png" alt="alt text" style="zoom:50%;" />
</p>


softmaxå¯¼æ•°åˆ†æï¼š

<p align="center">
    <img src="https://hhhi21g.github.io/public/img/pic342.png" alt="alt text" style="zoom:50%;" />
</p>


(y<sub>1~q</sub>=1)

<p align="center">
    <img src="https://hhhi21g.github.io/public/img/pic343.png" alt="alt text" style="zoom:50%;" />
</p>


------

<br/>

<br/>

#### 3.5  å›¾åƒåˆ†ç±»æ•°æ®é›†

import:

```python
import torch

from d2l import torch as d2l
from torch.utils import data
from torchvision import transforms
import torchvision
```

##### ç»„ä»¶ä»¬

ä¿å­˜è‡³d2lï¼š

```python
# æ ¹æ®æ•°å­—è¿”å›æ•°æ®é›†çš„æ–‡æœ¬æ ‡ç­¾
def get_fashion_mnist_labels(labels):  # @save
    text_labels = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat',
                   'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']
    return [text_labels[int(i)] for i in labels]
```

```python
# ç»˜åˆ¶å›¾åƒåˆ—è¡¨
def show_images(imgs, num_rows, num_cols, titles=None, scale=1.5):  #@save
    figsize = (num_cols * scale, num_rows * scale)
    _, axes = d2l.plt.subplots(num_rows, num_cols, figsize=figsize)
    axes = axes.flatten()
    for i, (ax, img) in enumerate(zip(axes, imgs)):
        if torch.is_tensor(img):
            # å›¾ç‰‡å¼ é‡
            ax.imshow(img.numpy())
        else:
            # PILå›¾ç‰‡
            ax.imshow(img)
        ax.axes.get_xaxis().set_visible(False)
        ax.axes.get_yaxis().set_visible(False)
        if titles:
            ax.set_title(titles[i])
    return axes
```

```python
# ä½¿ç”¨4ä¸ªè¿›ç¨‹æ¥è¯»å–æ•°æ®
def get_dataloader_workers():  # @save
    return 4
```

ç®€è¦æµç¨‹ï¼š

- ä¸‹è½½å¹¶è¯»å–æ•°æ®é›†

  ```python
  # ToTensorå°†å›¾åƒæ•°æ®ä»PILç±»å‹å˜æ¢æˆ32ä½æµ®ç‚¹æ•°æ ¼å¼
  # å¹¶é™¤ä»¥255ä½¿å¾—æ‰€æœ‰åƒç´ çš„æ•°å€¼å‡åœ¨0åˆ°1
  trans = transforms.ToTensor()
  mnist_train = torchvision.datasets.FashionMNIST(
      root='../data', train=True, transform=trans, download=True
  )
  mnist_test = torchvision.datasets.FashionMNIST(
      root='../data', train=False, transform=trans, download=True
  )
  ```

- æ•°æ®é›†çš„ç‰¹å¾

  ```python
  print(len(mnist_train))
  print(len(mnist_test))
  # æ¯ä¸ªè¾“å…¥å›¾åƒçš„é«˜åº¦å’Œå®½åº¦å‡ä¸º28åƒç´ ï¼Œæ•°æ®é›†ç”±ç°åº¦å›¾åƒç»„æˆï¼Œé€šé“æ•°ä¸º1
  print(mnist_train[0][0].shape) 
  ```

  > 60000<br/>
  > 10000<br/>
  > torch.Size([1, 28, 28])</br>

- è®­ç»ƒæ•°æ®é›†ä¸­å–æ ·å¹¶ç»˜å›¾

  ```python
  X, y = next(iter(data.DataLoader(mnist_train, batch_size=18)))
  show_images(X.reshape(18, 28, 28), 2, 9, titles=get_fashion_mnist_labels(y));
  d2l.plt.show()
  ```

- æ‰“ä¹±æ‰€æœ‰æ ·æœ¬è¯»å–å°æ‰¹é‡

  ```python
  batch_size = 256
  
  train_iter = data.DataLoader(mnist_train, batch_size, shuffle=True,
                               num_workers=get_dataloader_workers())
  ```

------

##### æ•´åˆç»„ä»¶

ä¿å­˜è‡³d2l:

```python
def load_data_fashion_mnist(batch_size, resize=None):  # @save
    """ä¸‹è½½Fashion-MNISTæ•°æ®é›†ï¼Œç„¶åå°†å…¶åŠ è½½åˆ°å†…å­˜ä¸­"""
    trans = [transforms.ToTensor()]
    if resize:
        trans.insert(0, transforms.Resize(resize))
    trans = transforms.Compose(trans)
    mnist_train = torchvision.datasets.FashionMNIST(
        root="../data", train=True, transform=trans, download=True)
    mnist_test = torchvision.datasets.FashionMNIST(
        root="../data", train=False, transform=trans, download=True)
    return (data.DataLoader(mnist_train, batch_size, shuffle=True,
                            num_workers=get_dataloader_workers()),
            data.DataLoader(mnist_test, batch_size, shuffle=False,
                            num_workers=get_dataloader_workers()))
```

æŒ‡å®šresizeå‚æ•°æ¥æµ‹è¯•å‡½æ•°çš„å›¾åƒå¤§å°è°ƒæ•´åŠŸèƒ½ï¼š

```python
if __name__ == "__main__":  # å¤šè¿›ç¨‹ä½¿ç”¨ï¼Œç¡®ä¿åªæœ‰ä¸»è¿›ç¨‹æ‰ä¼šæ‰§è¡Œä¸‹é¢ä»£ç 
    train_iter, test_iter = load_data_fashion_mnist(32, resize=64)
    for X, y in train_iter:
        print(X.shape, X.dtype, y.shape, y.dtype)
        break
```

> torch.Size([32, 1, 64, 64]) torch.float32 torch.Size([32]) torch.int64

------

<br/>

<br/>

#### 3.6  softmaxå›å½’çš„ä»é›¶å¼€å§‹å®ç°

import:

```python
import torch
from IPython import display
from d2l import torch as d2l
```

##### ä¿å­˜è‡³d2l

```python
# è®¡ç®—é¢„æµ‹æ­£ç¡®çš„æ•°é‡
def accuracy(y_hat, y):  # @ save
    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:
        y_hat = y_hat.argmax(axis=1)
    cmp = y_hat.type(y.dtype) == y
    return float(cmp.type(y.dtype).sum())
```

```python
def evaluate_accuracy(net, data_iter):  # @save
    """è®¡ç®—åœ¨æŒ‡å®šæ•°æ®é›†ä¸Šæ¨¡å‹çš„ç²¾åº¦"""
    if isinstance(net, torch.nn.Module):
        net.eval()  # å°†æ¨¡å‹è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    metric = Accumulator(2)  # æ­£ç¡®é¢„æµ‹æ•°ã€é¢„æµ‹æ€»æ•°
    with torch.no_grad():
        for X, y in data_iter:
            metric.add(accuracy(net(X), y), y.numel())
    return metric[0] / metric[1]
```

```python
class Accumulator:  # @save
    """åœ¨nä¸ªå˜é‡ä¸Šç´¯åŠ """

    def __init__(self, n):
        self.data = [0.0] * n

    def add(self, *args):
        self.data = [a + float(b) for a, b in zip(self.data, args)]

    def reset(self):
        self.data = [0.0] * len(self.data)

    def __getitem__(self, idx):
        return self.data[idx]
```

```python
def train_epoch_ch3(net, train_iter, loss, updater):  # @save
    """è®­ç»ƒæ¨¡å‹ä¸€ä¸ªè¿­ä»£å‘¨æœŸï¼ˆå®šä¹‰è§ç¬¬3ç« ï¼‰"""
    # å°†æ¨¡å‹è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼
    if isinstance(net, torch.nn.Module):
        net.train()
    # è®­ç»ƒæŸå¤±æ€»å’Œã€è®­ç»ƒå‡†ç¡®åº¦æ€»å’Œã€æ ·æœ¬æ•°
    metric = Accumulator(3)
    for X, y in train_iter:
        # è®¡ç®—æ¢¯åº¦å¹¶æ›´æ–°å‚æ•°
        y_hat = net(X)
        l = loss(y_hat, y)
        if isinstance(updater, torch.optim.Optimizer):
            # ä½¿ç”¨PyTorchå†…ç½®çš„ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
            updater.zero_grad()
            l.mean().backward()
            updater.step()
        else:
            # ä½¿ç”¨å®šåˆ¶çš„ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
            l.sum().backward()
            updater(X.shape[0])
        metric.add(float(l.sum()), accuracy(y_hat, y), y.numel())
    # è¿”å›è®­ç»ƒæŸå¤±å’Œè®­ç»ƒç²¾åº¦
    return metric[0] / metric[2], metric[1] / metric[2]
```

```python
class Animator:  # @save
    """åœ¨åŠ¨ç”»ä¸­ç»˜åˆ¶æ•°æ®"""

    def __init__(self, xlabel=None, ylabel=None, legend=None, xlim=None,
                 ylim=None, xscale='linear', yscale='linear',
                 fmts=('-', 'm--', 'g-.', 'r:'), nrows=1, ncols=1,
                 figsize=(3.5, 2.5)):
        # å¢é‡åœ°ç»˜åˆ¶å¤šæ¡çº¿
        if legend is None:
            legend = []
        d2l.use_svg_display()
        self.fig, self.axes = d2l.plt.subplots(nrows, ncols, figsize=figsize)
        if nrows * ncols == 1:
            self.axes = [self.axes, ]
        # ä½¿ç”¨lambdaå‡½æ•°æ•è·å‚æ•°
        self.config_axes = lambda: d2l.set_axes(
            self.axes[0], xlabel, ylabel, xlim, ylim, xscale, yscale, legend)
        self.X, self.Y, self.fmts = None, None, fmts
```

```python
def train_ch3(net, train_iter, test_iter, loss, num_epochs, updater):  # @save
    """è®­ç»ƒæ¨¡å‹ï¼ˆå®šä¹‰è§ç¬¬3ç« ï¼‰"""
    animator = Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0.3, 0.9],
                        legend=['train loss', 'train acc', 'test acc'])
    for epoch in range(num_epochs):
        train_metrics = train_epoch_ch3(net, train_iter, loss, updater)
        test_acc = evaluate_accuracy(net, test_iter)
        animator.add(epoch + 1, train_metrics + (test_acc,))
    train_loss, train_acc = train_metrics
    assert train_loss < 0.5, train_loss
    assert train_acc <= 1 and train_acc > 0.7, train_acc
    assert test_acc <= 1 and test_acc > 0.7, test_acc
```

```python
def predict_ch3(net, test_iter, n=6):  # @save
    """é¢„æµ‹æ ‡ç­¾ï¼ˆå®šä¹‰è§ç¬¬3ç« ï¼‰"""
    for X, y in test_iter:
        break
    trues = d2l.get_fashion_mnist_labels(y)
    preds = d2l.get_fashion_mnist_labels(net(X).argmax(axis=1))
    titles = [true + '\n' + pred for true, pred in zip(trues, preds)]
    d2l.show_images(
        X[0:n].reshape((n, 28, 28)), 1, n, titles=titles[0:n])
```

##### ä½¿ç”¨å®ä¾‹

```python
if __name__ == '__main__':
    # å®šä¹‰äº¤å‰ç†µæŸå¤±å‡½æ•°
    y = torch.tensor([0, 1, 1])
    y_hat = torch.tensor([[0.5, 0.2, 0.3], [0.1, 0.4, 0.5], [0.6, 0.3, 0.1]])

    print(y_hat[[0, 1, 2], y])

    print(cross_entropy(y_hat, y))
    print(accuracy(y_hat, y) / len(y))

    # ä½¿ç”¨éšæœºæƒé‡åˆå§‹åŒ–netæ¨¡å‹ï¼Œæ¨¡å‹ç²¾åº¦æ¥è¿‘äºéšæœºçŒœæµ‹ï¼Œè¿™é‡Œä¸ºæ¥è¿‘0.1
    print(evaluate_accuracy(net, test_iter))

    lr = 0.1

    def updater(batch_size):
        return d2l.sgd([w, b], lr, batch_size)


    num_epochs = 10
    train_ch3(net, train_iter, test_iter, cross_entropy, num_epochs, updater)


    predict_ch3(net, test_iter)
    d2l.plt.show()
```

è¾“å‡ºç»“æœ

> tensor([0.5000, 0.4000, 0.3000])<br/>
> tensor([0.6931, 0.9163, 1.2040])<br/>
> 0.3333333333333333<br/>
> 0.1345<br/>

------

<br/>

<br/>

#### 3.7  softmaxå›å½’çš„ç®€æ´å®ç°

##### æŸå¤±å‡½æ•°çš„æ•°å­¦åŸç†

<p align="center">
    <img src="https://hhhi21g.github.io/public/img/pic370.png" alt="alt text" style="zoom:67%;" />
</p>

<p align="center">
    <img src="https://hhhi21g.github.io/public/img/pic371.png" alt="alt text" style="zoom:67%;" />
</p>


------

##### å®ç°

```python
import matplotlib.pyplot as plt
import torch
from torch import nn
from d2l import torch as d2l

batch_size = 256
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)

net = nn.Sequential(nn.Flatten(), nn.Linear(784, 10))
loss = nn.CrossEntropyLoss(reduction='none')
trainer = torch.optim.SGD(net.parameters(),lr=0.1)

num_epochs = 10
d2l.train_ch3(net,train_iter,test_iter,loss,num_epochs,trainer)
plt.show()
```

<p align="center">
    <img src="https://hhhi21g.github.io/public/img/pic372.png" alt="alt text" style="zoom:100%;" />
</p>


------

