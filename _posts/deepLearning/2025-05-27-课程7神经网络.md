---
layout: post

title: 机器学习第7讲——神经网络

date: 2025-05-27 12:49:23 +0900

categories: [深度学习]
---
- [机器学习的三要素](#机器学习的三要素)
- [名词们](#名词们)
  - [泛化错误](#泛化错误)
  - [正则化](#正则化regularization)
  - [偏差-方差分解](#偏差-方差分解bias-variance-decomposition)
- [数学基础](#数学基础)
  - [无穷范数](#无穷范数)
  - [链式法则](#链式法则)
  - [熵与交叉熵](#熵与交叉熵)
    - [熵](#熵)
    - [交叉熵](#交叉熵)
  - [Jensen不等式](#jensen不等式)
- [神经网络基础模型](#神经网络基础模型)
  - [异或问题](#异或问题)
    - [单层感知机与逻辑运算](#单层感知机与逻辑运算)
    - [双层感知机](#双层感知机)
  - [万能近似定理](#万能近似定理universal-approximation-theorem)
  - [前馈网络](#前馈网络)
  - [记忆网络](#记忆网络反馈网络)
  - [图网络](#图网络)
  - [前馈神经网络](#前馈神经网络feedforward-neural-networkfnn)
- [隐藏层——激活函数](#隐藏层激活函数)
  - [Sigmoid型函数](#sigmoid型函数)
    - [Logistic(Sigmoid)](#logisticsigmoid)
    - [Tanh](#tanh)
    - [Hard-Logistic](#hard-logistic)
    - [Hard-Tanh](#hard-tanh)
  - [ReLU函数及其扩展](#relu函数及其扩展)
    - [ReLU](#relu)
    - [带泄露的ReLU(Leaky ReLU)](#带泄露的reluleaky-relu)
    - [指数线性单元ELU](#指数线性单元elu)
    - [Softplus](#softplus)
- [输出单元](#输出单元)
  - [线性输出单元](#线性输出单元)
  - [Sigmoid单元](#sigmoid单元)
  - [Softmax单元](#softmax单元)
- [自动梯度计算](#自动梯度计算)
  - [自动微分](#自动微分automatic-differentiation)
  - [计算图](#计算图computational-graph)  
- [卷积神经网络](#卷积神经网络convolutional-neural-network-cnn)
  - [卷积计算](#卷积计算)
  - [卷积的动机](#卷积的动机)
  - [池化层](#池化层pooling-layer)
  - [CNN说明](#cnn说明)
    - [结构](#结构)
- [自编码器(Autoencoder)](#自编码器autoencoder)
  - [核心思想](#核心思想精华为中间隐含层部分即编码器与解码器之间的部分)
  - [进行流程](#进行流程)
    - [网络表达、第一次编解码训练](#网络表达第一次编解码训练)
    - [训练——逐层静态进行](#训练逐层静态进行)
  - [作用](#作用)
    - [作为特征提取器来使用](#作为特征提取器来使用)
- [循环神经网络(RNN)](#循环神经网络rnn)
  - [Seq2Seq任务](#seq2seq任务)
  - [网络记忆能力](#网络记忆能力)
  - [网络结构](#网络结构)
  - [计算公式](#计算公式)
  - [实际使用](#实际使用)
  - [随时间反向传播(BPTT)](#随时间反向传播back-propagation-through-time-bptt)
  - [长程依赖问题](#长程依赖问题)
- [长短期记忆网络(LSTM)](#长短期记忆网络long-short-term-memory-networkslstm)
  - [长期状态c的控制](#长期状态c的控制)
  - [核心思想](#核心思想)
  - [算法实现](#算法实现)
- [Transformer](#transformer)
  - [Tokenization](#tokenization)
  - [Attention机制](#attention机制)
    
### 机器学习的三要素

<p >
    <img src="https://hhhi21g.github.io/assets/img/deepLearning/deepLearning/d168.png" alt="alt text" style="zoom:60%;" />
</p>

****

### 名词们

#### 泛化错误

- 一般表现为一个模在训练集和测试集上的错误率；

- 可以衡量一个机器学习模型是否可以很好地泛化到未知数据；

#### 正则化(regularization)

<p >
    <img src="https://hhhi21g.github.io/assets/img/deepLearning/deepLearning/d169.png" alt="alt text" style="zoom:60%;" />
</p>

#### 偏差-方差分解(Bias-Variance Decomposition)

<p >
    <img src="https://hhhi21g.github.io/assets/img/deepLearning/deepLearning/d170.png" alt="alt text" style="zoom:60%;" />
</p>

- **偏差：**模型预测值与真实值的平均偏离程度；偏差高，欠拟合；
- **方差：**模型对训练集的敏感程度；方差高，过拟合；

<p >
    <img src="https://hhhi21g.github.io/assets/img/deepLearning/deepLearning/d171.png" alt="alt text" style="zoom:60%;" />
</p>

****

### 数学基础

#### 无穷范数

<p >
    <img src="https://hhhi21g.github.io/assets/img/deepLearning/deepLearning/d172.png" alt="alt text" style="zoom:60%;" />
</p>

#### 链式法则

<p >
    <img src="https://hhhi21g.github.io/assets/img/deepLearning/deepLearning/d173.png" alt="alt text" style="zoom:60%;" />
</p>

#### 熵与交叉熵

##### 熵

<p >
    <img src="https://hhhi21g.github.io/assets/img/deepLearning/deepLearning/d175.png" alt="alt text" style="zoom:60%;" />
</p>

- 熵越高，则随机变量的信息越多

**计算实例：**

<p >
    <img src="https://hhhi21g.github.io/assets/img/deepLearning/deepLearning/d176.png" alt="alt text" style="zoom:60%;" />
</p>

##### 交叉熵

<p >
    <img src="https://hhhi21g.github.io/assets/img/deepLearning/deepLearning/d177.png" alt="alt text" style="zoom:60%;" />
</p>

- 交叉熵是按照概率分布q的最优编码对真实分布为p的信息进行编码的长度；
- 在给定q的情况下，如果p和q越接近，交叉熵越小；
- 如果p和q越远，交叉熵就越大。

**计算实例：**

<p >
    <img src="https://hhhi21g.github.io/assets/img/deepLearning/deepLearning/d178.png" alt="alt text" style="zoom:60%;" />
</p>

#### Jensen不等式

<p >
    <img src="https://hhhi21g.github.io/assets/img/deepLearning/deepLearning/d174.png" alt="alt text" style="zoom:60%;" />
</p>

****

### 神经网络基础模型

#### 异或问题

##### 单层感知机与逻辑运算

**阶跃激活函数：**线性组合的结果超过阈值，就输出1；否则输出0。

感知机是一个可以模拟逻辑运算(AND, OR, NOT)的线性分类器。

举例AND说明：

<p >
    <img src="https://hhhi21g.github.io/assets/img/deepLearning/deepLearning/d179.png" alt="alt text" style="zoom:60%;" />
</p>


图像：

<p >
    <img src="https://hhhi21g.github.io/assets/img/deepLearning/deepLearning/d180.png" alt="alt text" style="zoom:60%;" />
</p>

##### 双层感知机

单层感知机无法解决**异或**问题

<p >
    <img src="https://hhhi21g.github.io/assets/img/deepLearning/deepLearning/d181.png" alt="alt text" style="zoom:60%;" />
</p>

<p >
    <img src="https://hhhi21g.github.io/assets/img/deepLearning/deepLearning/d182.png" alt="alt text" style="zoom:60%;" />
</p>

****

#### 万能近似定理(universal approximation theorem)

即，一个两层人工神经网络如果具有足够多的隐藏单元，它可以以任意精度来近似任何一个函数

<p >
    <img src="https://hhhi21g.github.io/assets/img/deepLearning/deepLearning/d183.png" alt="alt text" style="zoom:60%;" />
</p>

- **单隐层网络**可以近似任何函数，但其规模可能巨大：

  最坏的情况下，需要**指数级的隐藏单元**才能近似某个函数；

- 随着深度的增加，网络的表示能力**呈指数增加**：

  具有d个输入、深度为l、每个隐藏层具有n个单元的深度整流网络可以描述的线性区域的数量为：

  <p >
  <img src="https://hhhi21g.github.io/assets/img/deepLearning/deepLearning/d184.png" alt="alt text" style="zoom:60%;" />
  </p>

- 更深层的网络具有更好的泛化能力。

- 参数数量的增加未必一定会带来模型效果的提升：
  - 更深的模型表现的更好，不仅仅是因为模型更大；
  - 想要学得的函数应该由许多**更简单的函数复合**在一起而得到。

****

#### 前馈网络

<p >
<img src="https://hhhi21g.github.io/assets/img/deepLearning/deepLearning/d185.png" alt="alt text" style="zoom:60%;" />
</p>

- 各个神经元按照接收信息的先后分成不同的组，每一组可看作一个神经层；
- 每一层的神经元接收来自前一层神经元的输出，并输出给下一层神经元；

- 整个网络中信息**朝一个方向传播**，**没有反向的信息传播**

****

#### 记忆网络(反馈网络)

<p >
<img src="https://hhhi21g.github.io/assets/img/deepLearning/deepLearning/d186.png" alt="alt text" style="zoom:60%;" />
</p>

- 神经元不但可以接收其他神经元的信息，也可以**接收自己的历史信息；**
- 神经元具有记忆功能，在不同时刻具有不同状态；
- 信息传播可以是**单向或者双向传递**，可用一个有向循环图或无向图来表示。

****

#### 图网络

<p >
<img src="https://hhhi21g.github.io/assets/img/deepLearning/deepLearning/d187.png" alt="alt text" style="zoom:60%;" />
</p>

- 定义在**图结构数据**上的神经网络；
- 图中每个结点都是由**一个或者一组神经元**构成；
- 结点之间的连接可以是有向的，也可以是无向的；
- 每个结点可以接收来自相邻结点或者自身的信息；
- 图网络是前馈网络和记忆网络的方法，包含许多不同的实现方式。

****

**除上之外，还有：增加跳跃连接，改变层与层之间的连接方式**

<p >
<img src="https://hhhi21g.github.io/assets/img/deepLearning/deepLearning/d188.png" alt="alt text" style="zoom:60%;" />
</p>

<p >
<img src="https://hhhi21g.github.io/assets/img/deepLearning/deepLearning/d189.png" alt="alt text" style="zoom:60%;" />
</p>

****

#### 前馈神经网络(Feedforward Neural Network,FNN)

即多层感知机(Multi-Layer Perception, MLP), 但此叫法并不十分合理，因为激活函数并不是感知机所采用的不连续阶跃函数。

<p >
<img src="https://hhhi21g.github.io/assets/img/deepLearning/deepLearning/d190.png" alt="alt text" style="zoom:60%;" />
</p>

- 通常用z表示原始输出，a表示激活函数后的输出。

<p >
<img src="https://hhhi21g.github.io/assets/img/deepLearning/deepLearning/d191.png" alt="alt text" style="zoom:60%;" />
</p>

****

### 隐藏层——激活函数

**性质要求：**

- 连续并可导的非线性函数, 允许少数点不可导；
- 激活函数及其导函数要尽可能简单；
- **导函数的值域要在一个合适的区间内**，不能太大也不能太小，否则会影响训练的效率和稳定性。

#### Sigmoid型函数

指一类S型曲线函数，为两端饱和函数。

> 两端饱和函数：当输入趋近于正无穷和负无穷时，输出会趋近于某个固定值

##### Logistic(Sigmoid)

<p >
<img src="https://hhhi21g.github.io/assets/img/deepLearning/deepLearning/d192.png" alt="alt text" style="zoom:60%;" />
</p>

<p >
<img src="https://hhhi21g.github.io/assets/img/deepLearning/deepLearning/d193.png" alt="alt text" style="zoom:60%;" />
</p>

- 具有”挤压“功能；
- 输出看作是概率分布。

##### Tanh

<p >
<img src="https://hhhi21g.github.io/assets/img/deepLearning/deepLearning/d194.png" alt="alt text" style="zoom:60%;" />
</p>

<p >
<img src="https://hhhi21g.github.io/assets/img/deepLearning/deepLearning/d195.png" alt="alt text" style="zoom:60%;" />
</p>

- 零中心化，可提升收敛速度。

##### Hard-Logistic

<p >
<img src="https://hhhi21g.github.io/assets/img/deepLearning/deepLearning/d196.png" alt="alt text" style="zoom:60%;" />
</p>

<p >
<img src="https://hhhi21g.github.io/assets/img/deepLearning/deepLearning/d197.png" alt="alt text" style="zoom:60%;" />
</p>

##### Hard-Tanh

<p >
<img src="https://hhhi21g.github.io/assets/img/deepLearning/deepLearning/d198.png" alt="alt text" style="zoom:60%;" />
</p>

<p >
<img src="https://hhhi21g.github.io/assets/img/deepLearning/deepLearning/d199.png" alt="alt text" style="zoom:60%;" />
</p>

- 对Logistic和Tanh函数的分段近似；
- 与Logistic和Tanh相比，降低了计算开销

****

#### ReLU函数及其扩展

##### ReLU

<p >
<img src="https://hhhi21g.github.io/assets/img/deepLearning/deepLearning/d200.png" alt="alt text" style="zoom:60%;" />
</p>

<p >
<img src="https://hhhi21g.github.io/assets/img/deepLearning/deepLearning/d201.png" alt="alt text" style="zoom:60%;" />
</p>

- 目前**最常用**的激活函数；
- 具有单侧抑制，宽兴奋边界的生物学合理性；
- 可**缓解梯度消**失问题；
- 缺点：有可能导致神经元的死亡

##### 带泄露的ReLU(Leaky ReLU)

<p >
<img src="https://hhhi21g.github.io/assets/img/deepLearning/deepLearning/d202.png" alt="alt text" style="zoom:60%;" />
</p>

<p >
<img src="https://hhhi21g.github.io/assets/img/deepLearning/deepLearning/d203.png" alt="alt text" style="zoom:60%;" />
</p>

- 在x<0也保持一个很小的梯度，避免永远不能被激活的情况；
- γ：超参

##### 指数线性单元ELU

<p >
<img src="https://hhhi21g.github.io/assets/img/deepLearning/deepLearning/d204.png" alt="alt text" style="zoom:60%;" />
</p>

<p >
<img src="https://hhhi21g.github.io/assets/img/deepLearning/deepLearning/d205.png" alt="alt text" style="zoom:60%;" />
</p>

- 近似零中心化；

##### Softplus

<p >
<img src="https://hhhi21g.github.io/assets/img/deepLearning/deepLearning/d206.png" alt="alt text" style="zoom:60%;" />
</p>

<p >
<img src="https://hhhi21g.github.io/assets/img/deepLearning/deepLearning/d207.png" alt="alt text" style="zoom:60%;" />
</p>

- 可看作ReLU的平滑版本；
- 其**导数刚好为Logistic函数**；
- 具有单侧抑制，宽兴奋边界的特性；
- 但不具有稀疏性

****

### 输出单元

#### 线性输出单元

<p >
<img src="https://hhhi21g.github.io/assets/img/deepLearning/deepLearning/d208.png" alt="alt text" style="zoom:60%;" />
</p>

- 常用于条件高斯分布的均值；
- 适合连续值预测(回归)问题

- 可采用**均方误差损失函数**：

  <p >
  <img src="https://hhhi21g.github.io/assets/img/deepLearning/deepLearning/d209.png" alt="alt text" style="zoom:60%;" />
  </p>

#### Sigmoid单元

<p >
<img src="https://hhhi21g.github.io/assets/img/deepLearning/deepLearning/d210.png" alt="alt text" style="zoom:60%;" />
</p>

- 常用于输出Bernoulli分布；

- 适合二分类问题；

- 可采用**交叉熵损失函数**：

  <p >
  <img src="https://hhhi21g.github.io/assets/img/deepLearning/deepLearning/d211.png" alt="alt text" style="zoom:60%;" />
  </p>

#### Softmax单元

<p >
<img src="https://hhhi21g.github.io/assets/img/deepLearning/deepLearning/d212.png" alt="alt text" style="zoom:60%;" />
</p>

- 常用于输出Multimoulli分布；
- 适合多分类问题

- 可采用**交叉熵函数**：

  <p >
  <img src="https://hhhi21g.github.io/assets/img/deepLearning/deepLearning/d213.png" alt="alt text" style="zoom:60%;" />
  </p>

****

### 自动梯度计算

#### 自动微分(Automatic Differentiation)

自动微分将符号微分法应用于最基本的算子，比如常数、幂函数、指数函数等，将其代入数值，保留中间结果，最后再应用于整个函数

- 微分求解过程对用户是透明的；
- 不需要专门的数学语言和编程；
- 采用图的方式进行计算，可以做很多优化

#### 计算图(Computational Graph)

- 将复合函数分解为一系列基本操作，并以图的形式连接起来；
- 是数学运算的图结构表示，每个非叶子结点代表一个基本操作，每个叶子结点代表一个输入变量或常量

**实例：**

<p >
<img src="https://hhhi21g.github.io/assets/img/deepLearning/deepLearning/d214.png" alt="alt text" style="zoom:60%;" />
</p>

<p >
<img src="https://hhhi21g.github.io/assets/img/deepLearning/deepLearning/d215.png" alt="alt text" style="zoom:60%;" />
</p>
**静态计算图(Theano, Tensorflow)：**

- 编译时构建计算图，构建好后在程序运行时不能改变；
- 构建时可以进行优化、并行能力强；
- 灵活性较差

**动态计算图(PyTorch, Tensorflow 2.0)：**

- 程序运行时动态构建计算图；
- 不容易优化，当不同输入所使用网络结构不一样时，难以并行计算；
- 灵活性比较高

****

### 卷积神经网络(Convolutional Neural Network, CNN)

——一般是由卷积层、池化层和全连接层交叉堆叠而成的前馈神经网络

**3个特性：**局部连接、权重共享、池化

使用全连接前馈网络来处理图像时，存在以下两个问题：

- **参数过多：**如果输入图像为100 x 100 x 3，则第一个隐藏层的每个神经元到输入层都有30,000个互相独立的连接；这会导致整个神经网络的训练效率非常低，也很容易出现过拟合；

- **局部不变性特征：**比如尺度缩放、平移、旋转等操作不影响其语义信息，而全连接前馈网络很难提取这些局部不变性特征，一般需要进行数据增强来增强性能。

**感受野(Receptive Field)机制:** 主要时指听觉、视觉等神经系统中一些神经元的特性，即神经元只接收其所支配的刺激区域内的信号。

#### 卷积计算

神经网络中使用卷积是为了进行特征提取，卷积核是否进行翻转和其特征抽取能力无关。为了实现上的方便起见，使用互相关来代替卷积。

- 步长(Stride): 卷积核在滑动时的时间间隔；
- 零填充(Zero Padding): 在输入向量两端进行补零

<p >
<img src="https://hhhi21g.github.io/assets/img/deepLearning/deepLearning/d216.png" alt="alt text" style="zoom:60%;" />
</p>

##### 1D

- 通常用于**信号处理**中，用于计算信号的延迟累积；

- 假设一个信号发生器每个时刻t产生一个信号x<sub>t</sub>, 其信息衰减率为w<sub>k</sub>， 即在k - 1个时间步长后，信息为原来的w<sub>k</sub>倍，

<p >
<img src="https://hhhi21g.github.io/assets/img/deepLearning/deepLearning/d217.png" alt="alt text" style="zoom:60%;" />
</p>

> F：卷积核长度；注意i：Padding为在原向量两边都填充等量的0

##### 2D

- 在图像处理中，卷积经常作为特征提取的有效方法；
- 一幅图像在经过卷积操作后得到的结果称为特征映射(Feature Map)

- 常用的滤波器：第一种为高斯滤波器，可以对图像进行平滑去噪；中间和下面的滤波器可以用来提取边缘特征

<p >
<img src="https://hhhi21g.github.io/assets/img/deepLearning/deepLearning/d218.png" alt="alt text" style="zoom:60%;" />
</p>

<p >
<img src="https://hhhi21g.github.io/assets/img/deepLearning/deepLearning/d219.png" alt="alt text" style="zoom:60%;" />
</p>

> - W2，H2: 卷积后的宽度和高度；\n
> - F： 滤波器的宽度；\n
> - P：零填充的数量；\n
> - S：步长

#### 卷积的动机

- 每一个神经元都只和下一层中**某个局部窗口内**的神经元相连，构成一个局部连接网络。

- **权值共享：**每个神经元可以使用同一个卷积核去卷积图像，即一个卷积核**只捕捉输入数据中的一种局部特征，**后果是**只提取了一种特征**
  - 如需要提取不同的特征：多加几种滤波器

- **平移不变性：**卷积+池化共同实现
  - 卷积保证仍然能检测到它的特征；
  - 池化尽可能地保持一致的表达。

**仍存在问题？**

卷积层虽然可以**显著减少网络中连接的数量**，但特征映射组中的**神经元个数并没有显著减少**，容易出现过拟合；

为解决该问题，可以在卷积层之后加上一个**池化层**，从而降低特征维度

****

#### 池化层(Pooling Layer)

也叫子采样层(Subsampling Layer), 作用是进行特征选择，降低特征数量，从而减少参数数量

##### 池化

指对每个区域进行下采样(Down Sampling)得到一个值，作为这个区域的概括

<p >
<img src="https://hhhi21g.github.io/assets/img/deepLearning/deepLearning/d220.png" alt="alt text" style="zoom:60%;" />
</p>

**平均池化：**主要用来抑制临域值之间差别过大，造成的方差过大

- CV中，对背景的保留效果好

**最大池化：**能够抑制网络参数误差造成的估计均值偏移的现象

如，输入(1,5,3),最大池化后是(5);

假如输入中的参数1，有了误差变成1.5，这时输入是(1.5,5,3),最大池化后结果还是(5)

- CV中，对纹理的提取较好

****

#### CNN说明

##### 结构

<p >
<img src="https://hhhi21g.github.io/assets/img/deepLearning/deepLearning/d222.png" alt="alt text" style="zoom:60%;" />
</p>

<p >
<img src="https://hhhi21g.github.io/assets/img/deepLearning/deepLearning/d223.png" alt="alt text" style="zoom:60%;" />
</p>

<p >
<img src="https://hhhi21g.github.io/assets/img/deepLearning/deepLearning/d221.png" alt="alt text" style="zoom:60%;" />
</p>

- 浅层学到的特征为简单的边缘、角点、纹理、几何形状等；
- 深层学到的特征则更为复杂抽象，为狗、人脸等。

- 卷积神经网络每层的卷积核权重是由数据驱动学习得来，人工只能胜任简单卷积核的设计

****

### 自编码器(Autoencoder)

CNN的训练样本是带有标签的；在很多实际应用中，训练样本是没有标签的；应用误差反向传播算法来训练网络会遇到一些困难。

一种自然地扩展是在整个网络中**让输出与输入相等**。在此网络中，**隐含层则可以理解为用于记录数据的特征**，因此这是一种典型的**表示学习方法。**

**自动编码器是一种尽可能重构输入信号的神经网络**

#### 核心思想：精华为中间隐含层部分，即编码器与解码器之间的部分

- 将input输入一个encoder编码器，就会得到一个code, 也**即输入的一个表示**；

- 通过增加一个decoder解码器，并采用**信号重构**的方式来评价这个code的质量；

- 理想情况下，希望decoder所输出的信息(**表达可能不一样，但本质上反映的是同一种模式**)，与输入信号input是相同的；

- 此时会产生误差，我们**期望这个误差最小。**

  <p >
  <img src="https://hhhi21g.github.io/assets/img/deepLearning/deepLearning/d224.png" alt="alt text" style="zoom:60%;" />
  </p>

为了重构原始信号，就像主成分分析，它必须学习(捕捉)到可以代表输入数据的最重要的内在因素

****

#### 进行流程

##### 网络表达、第一次编解码训练

- **编码：**建立输入层至隐含层的权重；
- **Code:** 隐含层的输出，也称为表达或特征；
- **解码：**建立隐含层至输出层的权重。

##### 训练——逐层静态进行

<p >
<img src="https://hhhi21g.github.io/assets/img/deepLearning/deepLearning/d225.png" alt="alt text" style="zoom:60%;" />
</p>

- 对每个样本，将第一层输出的code当成第二层的输入信号，**再次利用一个新的三层前向神经网络来最小化重构误差**，得到第二层的权重参数；
- 同样得到样本在该层的code，即原始号的第二个表达；
- 当训练当前层时，其他层固定不动；完成当前编码和解码任务；前一次”编码“和”解码“均不考虑；
- 这一过程实质上是一个静态的堆叠(stack)过程；
- 每次训练可以用**BP算法对一个三层前向网络**进行训练。

****

#### 作用

##### 作为特征提取器来使用

只应用编码器：样本逐步通过多个编码器，然后以**最后一层编码器的输出作为该样本的新特征**

<p >
<img src="https://hhhi21g.github.io/assets/img/deepLearning/deepLearning/d226.png" alt="alt text" style="zoom:60%;" />
</p>

- 对于分类任务，可以事先用Autoencoder对数据进行学习。然后以得到的权值作为初始权重，采用带有标签的数据对网络进行再次学习，即fine-tuning技术；
- 为了实现分类任务，需要在编码阶段的最后一层加上一个**分类器**，比如一个多层感知机

****

### 循环神经网络(RNN)

#### Seq2Seq任务

**指输入和输出都是序列的任务，输出的长度不确定时采用的模型；**

一般在机器翻译的任务中出现：翻译后的文本长度不确定。

#### 网络记忆能力

全连接神经网络和卷积神经网络：只能单独的处理一个个的输入，前一个输入和后一个输入是完全没有关系的。

某些任务需要能够更好地**处理序列的信息**：前面的输入和后面的输入是有关系的，比如，理解一句话，分析一段视频。**因此我们希望神经网络拥有“记忆”的能力，能够通过之前的信息得到不同输出。**(实例PP11)

****

#### 网络结构

<p >
<img src="https://hhhi21g.github.io/assets/img/deepLearning/deepLearning/d227.png" alt="alt text" style="zoom:60%;" />
</p>

- s是一个向量，表示隐藏层的值；s不仅仅取决于当前这次的输入x，**还取决于上一次隐藏层的值；**

- U,V,W均为权重矩阵，其中W是隐藏层上一次的值作为这一次的输入的权重矩阵。

#### 计算公式

<p >
<img src="https://hhhi21g.github.io/assets/img/deepLearning/deepLearning/d228.png" alt="alt text" style="zoom:60%;" />
</p>

****

#### 实际使用

- 如何将可变长度序列作为输入？
- 如何预测可变长度序列作为输出？

##### 文本分类：输入单词序列，输出文本类别

<p >
<img src="https://hhhi21g.github.io/assets/img/deepLearning/deepLearning/d229.png" alt="alt text" style="zoom:60%;" />
</p>

##### 图片：输入图片，输出图片的文字描述

<p >
<img src="https://hhhi21g.github.io/assets/img/deepLearning/deepLearning/d230.png" alt="alt text" style="zoom:60%;" />
</p>

##### 视频：输入视频序列，输出每一帧对应的标签

<p >
<img src="https://hhhi21g.github.io/assets/img/deepLearning/deepLearning/d231.png" alt="alt text" style="zoom:60%;" />
</p>

##### 机器翻译(Encoder-Decoder, Seq2Seq)

<p >
<img src="https://hhhi21g.github.io/assets/img/deepLearning/deepLearning/d232.png" alt="alt text" style="zoom:60%;" />
</p>

****

#### 随时间反向传播(Back-Propagation Through Time, BPTT)

**常用的训练RNN的方法**：因为RNN处理时间序列数据，所以要基于时间反向传播，故称。

本质上还是BP算法

<p >
<img src="https://hhhi21g.github.io/assets/img/deepLearning/deepLearning/d233.png" alt="alt text" style="zoom:60%;" />
</p>

由上图可知，需要寻优的参数有3个，UVW；

**BPTT算法的重点**：权重矩阵W和U的寻优过程需要追溯之前的历史数据。

<p >
<img src="https://hhhi21g.github.io/assets/img/deepLearning/deepLearning/d234.png" alt="alt text" style="zoom:60%;" />
</p>

****

#### 长程依赖问题

从某个时刻开始，前面的**梯度已经几乎减少到0**，即再往前走，得到的梯度不会对最终的梯度值有任何的贡献。这就是原始的RNN无法处理长距离依赖的原因。

****

### 长短期记忆网络(Long Short Term Memory networks,LSTM)

为解决长程依赖问题，一种特殊的RNN网络。

<p >
<img src="https://hhhi21g.github.io/assets/img/deepLearning/deepLearning/d235.png" alt="alt text" style="zoom:60%;" />
</p>

**增加状态c，称为单元状态(cell state), 让它来保存长期的状态**

#### 长期状态c的控制

<p >
<img src="https://hhhi21g.github.io/assets/img/deepLearning/deepLearning/d236.png" alt="alt text" style="zoom:60%;" />
</p>

- 第1个开关：控制如何继续保存长期状态c；
- 第2个开关：控制把即时状态输入到长期状态c；
- 第3个开关：控制是否把长期状态c作为当前LSTM的输出。

#### 核心思想

<p >
<img src="https://hhhi21g.github.io/assets/img/deepLearning/deepLearning/d237.png" alt="alt text" style="zoom:60%;" />
</p>

**LSTM的关键是单元状态，如水平线在图上方贯穿运行**

单元状态的传递类似于传送带，其直接在整个链上运行，中间只有一些少量的线性交互，容易保存相关信息。

#### 算法实现

“门”：一种让信息选择式通过的方法。

- **遗忘门(forget gate):** 决定上一时刻的单元状态有多少保留到当前时刻；
- **输入门(input gate):** 决定当前时刻网络的输入有多少保留到单元状态；
- **输出门(output gate):** 控制单元状态有多少输出到LSTM的当前输出值；

<p >
<img src="https://hhhi21g.github.io/assets/img/deepLearning/deepLearning/d238.png" alt="alt text" style="zoom:60%;" />
</p>

<p >
<img src="https://hhhi21g.github.io/assets/img/deepLearning/deepLearning/d239.png" alt="alt text" style="zoom:60%;" />
</p>

<p >
<img src="https://hhhi21g.github.io/assets/img/deepLearning/deepLearning/d240.png" alt="alt text" style="zoom:60%;" />
</p>

<p >
<img src="https://hhhi21g.github.io/assets/img/deepLearning/deepLearning/d241.png" alt="alt text" style="zoom:60%;" />
</p>

<p >
<img src="https://hhhi21g.github.io/assets/img/deepLearning/deepLearning/d242.png" alt="alt text" style="zoom:60%;" />
</p>

<p >
<img src="https://hhhi21g.github.io/assets/img/deepLearning/deepLearning/d243.png" alt="alt text" style="zoom:60%;" />
</p>

****

### Transformer

<p >
<img src="https://hhhi21g.github.io/assets/img/deepLearning/deepLearning/d244.png" alt="alt text" style="zoom:60%;" />
</p>

#### Tokenization

<p >
<img src="https://hhhi21g.github.io/assets/img/deepLearning/deepLearning/d245.png" alt="alt text" style="zoom:60%;" />
</p>

<p >
<img src="https://hhhi21g.github.io/assets/img/deepLearning/deepLearning/d246.png" alt="alt text" style="zoom:60%;" />
</p>

- **Token Embedding:** 训练时得到的，没有考虑上下文；
- **Positional Embedding: ** 可以在训练时得到的，每一个位置有一个独特向量，即Positional Embedding

二者结合，Transformer才能同时理解词义和顺序。

#### Attention机制

**找出相关token:**

<p >
<img src="https://hhhi21g.github.io/assets/img/deepLearning/deepLearning/d247.png" alt="alt text" style="zoom:60%;" />
</p>

**集合相关信息：**

<p >
<img src="https://hhhi21g.github.io/assets/img/deepLearning/deepLearning/d248.png" alt="alt text" style="zoom:60%;" />
</p>

****
