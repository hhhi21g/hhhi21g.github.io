---
layout: post

title: 信安杯demo2.py代码解读——Base on CardioCam

date: 2025-04-17 13:37:23 +0900

categories: [信安杯]
---

### 信安杯demo2.py代码解读——Base on CardioCam

代码链接：https://github.com/hhhi21g/XinAnBei/blob/main/heart/PCA/demo2.py

目前代码相当冗余，暂时仅考虑功能性

所需import:

```python
import librosa
import numpy as np
```

#### 1. 从wav心音文件中提取MFCC(梅尔频率倒谱系数)特征

```python
def extract_features_with_mfcc(wav_file, n_mfcc=15, hop_length=64):
    y, sr = librosa.load(wav_file, sr=None)

    # 提取MFCC特征
    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc, hop_length=hop_length)

    # 返回MFCC特征矩阵，即音频信号的一个低纬度特征向量
    return mfcc.T  # (时间帧数, 特征维度)
```

> wav_file: 音频文件的路径；<br/>
>
> n_mfcc=15：表示要提取15维的MFCC特征，通常设置为12-20；<br/>
>
> hop_length=64:  在进行MFCC时，窗口的步长；表示每隔时间轴上的64个采样点提取一个新的音频帧；<br/>
>
> 步长越小，窗口的重叠越多，得到的时间分辨率越高，但计算量也越大。这里将其设置为64，速度很慢，能够正确分辨25HZ截至频率下的用户音频。

- 原始`mfcc`输出：每列是一个特征维度，行数对应不同的时间帧；
- **返回转置后的**`mfcc.T`: 每列是一个时间帧的特征向量，每行表示不同的特征维度。 

保持选取时间帧长度不变，更改hop_length：

>  测试结果为： data\lsr目录下audio(1)-audio(6)及night1-night6这12个样本的识别情况

<p>
    <img src="https://hhhi21g.github.io/assets/img/xinan/xinan0/x2.png" alt="alt text" style="zoom:67%;" />
</p>



------

#### 2.  处理返回的mfcc.T, 仅选取5000-15000时间帧部分进行后续处理

```python
max_frames = 10000  # 使用的时间帧长度
min_frames = 5000  # 选取的起始时间帧
```

```python
def cut_wav_file(extracted_matrix):
    if extracted_matrix.shape[0] > max_frames + min_frames:
        extracted_matrix = extracted_matrix[min_frames:min_frames + max_frames]
    return extracted_matrix
```

一分钟的心音音频具有时间帧为90000以上，实际应用需要的录制时间太长。

经尝试，**仅保留10000长度的时间帧**也足以获得特征，去掉头尾，实际录得20000时间帧，15(7)秒则足够(或许可以使用更短的时间，后续尝试)。

保持mfcc使用hop_length64进行测试：

<p>
    <img src="https://hhhi21g.github.io/assets/img/xinan/xinan0/x1.png" alt="alt text" style="zoom:67%;" />
</p>



------

#### 3.  创建三个档案音频信息，所有人、男 & 女

```python
user_gender = [0, 1, 0, 1]  # 存储用户性别,0女1男

wav_files_list = [
    ['..\\data\\xyt\\audio5.wav', '..\\data\\xyt\\audio15.wav', '..\\data\\xyt\\audio25.wav'],
    ['..\\data\\lshenr\\audio5.wav', '..\\data\\lshenr\\audio15.wav', '..\\data\\lshenr\\audio25.wav'],
    ['..\\data\\lsr\\audio5.wav', '..\\data\\lsr\\audio15.wav', '..\\data\\lsr\\audio25.wav'],
    ['..\\data\\lhb\\audio5.wav', '..\\data\\lhb\\audio10.wav', '..\\data\\lhb\\audio15.wav']
]

female_list = [wav_files_list[0], wav_files_list[2]]
male_list = [wav_files_list[1], wav_files_list[3]]

user_ids = ['xyt', 'lshenr', 'lsr', 'lhb']
female_ids = ['xyt', 'lsr']
male_ids = ['lshenr', 'lhb']
```

经计算，不同性别之间欧几里得距离差距较大，同性别之间差距较小，因此按性别分开。

------

#### 4. 进行奇异值分解(SVD)

```python
def SVD(user_list):
```

> user_list: 档案列表，及第3步中建立的wav_files_list, female_list, male_list其中之一

**part 1：获得生物特征矩阵**

```python
all_features = []
for user_files in user_list:
    user_feats = []
    for file in user_files:
        feat = extract_features_with_mfcc(file)
        feat = cut_wav_file(feat)
        user_feats.append(feat)
    # 合并用户所有音频的特征矩阵
    all_features.append(np.vstack(user_feats))

# 将多个用户的特征矩阵合并为一个生物特征矩阵
biometric_matrix = np.vstack(all_features)
```

- 处理传入列表内的所有音频文件：提取mfcc特征，统一剪切；
- 对于每一个用户，将对应的每个音频文件的处理结果矩阵append进user_feats;
- 将user_feats中的内容vstack(即verticle stack，按照垂直方向堆叠成一个矩阵), 将结果append进all_features, 即获得了一个用户的档案；
- 将all_features中的内容vstack, 即获得了一个矩阵**biometric_matrix，其中每一项为一个音频的档案，矩阵形状：[总音频数量，mfcc特征数]；**

**part 2:  数据中心化**

```python
# 数据中心化
mean = np.mean(biometric_matrix, axis=0)
biometric_matrix_centered = biometric_matrix - mean
```

- 沿axis=0, 即对于所有样本的每一个特征求均值；每个特征 - 该特征所有样本的均值，以使得数据的均值为0；

- 为什么中心化？去除均值影响

  如果数据中存在较大的均值偏移，**SVD会把数据的平均值作为一个主成分**，中心化可以去除均值对结果的影响，确保SVD只反映数据的主要变异性。

**part3: SVD**

```python
 U, sigma, VT = np.linalg.svd(biometric_matrix_centered, full_matrices=False)
```

`np.linalg.svd():` 对矩阵进行奇异值分解，SVD将矩阵分解为三个部分：**U(左奇异矩阵)，sigma(奇异值),  VT(右奇异矩阵)：**

- U:  正交矩阵，其中的列是左奇异向量，即数据的特征向量；[总音频数量, 总音频数量]

- sigma：对角矩阵；奇异值越大，说明对应的奇异向量(特征)越重要；[总音频数量，mfcc特征数]

- VT：正交矩阵，其中的行是右奇异向量；这部分包含了数据的主成分信息。[mfcc特征数，mfcc特征数]

  > 注：<br/>
  >
  > U每一列表示一个主成分方向，反映样本之间的相似性和结构, 即每一行的关系；<br/>
  >
  > VT每一行表示一个主成分方向，第一行表示最重要，后续依次递减；反映样本每一列之间的关系<br/>
  >
  > 主成分方向指：以VT[0]为例，VT[0] = [v0,v1,v2, ...... ,v14];<br/>
  >
  > 原矩阵有15列，每一列对应一个mfcc矩阵，写作[x0,x1, ...... ,x14];<br/>
  >
  > 即第一主成分：v0 * x0 + v1 * x1 + ...... + v14 * x14

**part4：主成分选择**

```python
# 主成分选择
normalized_variances = (sigma ** 2) / (sigma ** 2).sum()
sum_first_two = normalized_variances[:2].sum()
sum_rest = 1.0 - sum_first_two
required_sum = sum_rest * 0.9  # 累计剩余方差的90%，选择这些主成分

# 舍弃前两个主成分，从第三个开始选择
current_sum = 0.0
selected_indices = []
for idx in range(2, len(normalized_variances)):
    current_sum += normalized_variances[idx]
    selected_indices.append(idx)
    if current_sum >= required_sum:
    break
```

- `normalized_variances:` 将`sigma`的平方除以总和，得到每个主成分的方差占比；

- `sum_first_two:` 计算前两个主成分的方差总和；

  > 前两个主成分可能频率等个体性不强的特征，引入前两个特征使得样本的欧几里得距离接近，因此舍弃前两个主成分；<br/>
  >
  > 这里对于单一性别，可能需要使用其他范围的主成分进行区分，后续尝试；

- `required_sum:` 用于选取足以解释剩余方差90%的主成分；
- `selected_indices:` 选取的主成分.

**part5:  用户档案的构建**

```python
 # 构建用户档案
user_profiles = []
for user_files in user_list:
    user_feats = []
    for file in user_files:
        feat = extract_features_with_mfcc(file)
        feat = cut_wav_file(feat)
        user_feats.append(feat)
    user_feats_matrix = np.vstack(user_feats)
    # 中心化并投影
    transformed = (user_feats_matrix - mean) @ VT.T[:, selected_indices]
    user_profiles.append(np.mean(transformed, axis=0))
return mean, VT, selected_indices, user_profiles
```

- 跟part1部分重复，提取特征并进行**中心化处理**；
- **投影**到选中的主成分空间(与`VT.T[:, selected_indices]`相乘)；`transformed`表示经过降维处理的用户特征；
- 沿axis=0取平均值，即沿一个用户的所有时间帧，也即关于每个特征的所有时间，求平均值，得到该用户档案；append进user_profiles
- 返回：
  - mean：对于所有样本的每个特征求得的均值；
  - VT：15个主成分方向，按照重要程度降序排列；
  - selected_indices: 从第二个选取的，能够包含90%方差的主成分；
  - user_profiles:  用户档案，对于一个用户，其档案表示：每个被选择的特征在所有音频样本上的均值

------

#### 5.  处理新用户

```python
# 处理新用户
new_feat = extract_features_with_mfcc('..\\data\\lsr\\audio25.wav')
new_feat = cut_wav_file(new_feat)
mean, VT, selected_indices, user_profiles = SVD(wav_files_list)
new_transformed = (new_feat - mean) @ VT.T[:, selected_indices]
new_profile = np.mean(new_transformed, axis=0)
```

同4中的part5相同，建立一个新用户的档案

```python
# 计算欧几里得距离
distances = [np.linalg.norm(up - new_profile) for up in user_profiles]
threshold = 1.78  # 需要根据实际数据校准
```

样本数目前太少，threshold未定

```python
# 找到最小距离和对应的用户索引
min_distance = min(distances)
min_index = distances.index(min_distance)

print(distances)
print(min_distance)
print(min_index)
print(user_gender[min_index])
print("************************************************************************")
```

<p>
    <img src="https://hhhi21g.github.io/assets/img/xinan/xinan0/x0.png" alt="alt text" style="zoom:67%;" />
</p>

以一个女性新样本为例，其中已有用户档案中第1、3为女，2、4为男。可以看出，同一性别之间的欧式距离很接近，而不同性别之间差距较远；**现有代码能够很容易分辨出男女**。

------

#### 6.  进一步处理新用户，只处理其对应的性别

```python
if user_gender[min_index] == 0:  # 女
    mean_0, VT_0, selected_indices_0, user_profiles_0 = SVD(female_list)
    new_female_transformed = (new_feat - mean_0) @ VT_0.T[:, selected_indices_0]
    new_female_profile = np.mean(new_female_transformed, axis=0)
    distances_0 = [np.linalg.norm(up - new_female_profile) for up in user_profiles_0]
    new_min_distance = min(distances_0)
    print(distances_0)
    new_min_index = distances_0.index(new_min_distance)

if user_gender[min_index] == 1:  # 男
    mean_1, VT_1, selected_indices_1, user_profiles_1 = SVD(male_list)
    new_male_transformed = (new_feat - mean_1) @ VT_1.T[:, selected_indices_1]
    new_male_profile = np.mean(new_male_transformed, axis=0)
    distances_1 = [np.linalg.norm(up - new_male_profile) for up in user_profiles_1]
    print(distances_1)
    new_min_distance = min(distances_1)
    new_min_index = distances_1.index(new_min_distance)

print(new_min_distance)
print(new_min_index)

if new_min_index <= threshold:
    print("身份验证通过")
    if user_gender[min_index] == 0:
        print(female_ids[new_min_index])
    else:
        print(male_ids[new_min_index])
else:
    print("身份验证失败")
```

这一步将男女音频分开处理，初步想法是：在男女之间重要的主成分未必在同性别之间还很重要；因此想要将同性音频进行SVD处理，希望得到更能反映同性个体差异的主成分，但目前没什么效果。

------

