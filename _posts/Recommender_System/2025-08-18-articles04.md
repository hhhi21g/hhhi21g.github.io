---
layout: post

title: 04_GLINT-RU:Gated Lightweight Intelligent Recurrent Units for Sequential Recommender Systems

date: 2025-08-18 22:11:23 +0900

categories: [推荐系统]
tags: [论文阅读]
---

来源：KDD'25  https://dl.acm.org/doi/10.1145/3690624.3709304

代码：https://github.com/szhang-cityu/GLINT-RU

### 问题的引出

- Transformer-based models 在 Sequential Recommender Systems（SRSs）中很受欢迎，但计算开销大，推理速度慢；
- 现有efficient SRS难以在潜在表达中填充高质量的语义和位置信息；

提出**GLINT-RU**, **G**ated **L**ightweight **I**ntellige**NT** **R**ecurrent **U**nits.

****

### 准备工作

#### 1. Gated Recurrent Units(门控循环网络)

- 捕捉item序列之间的依赖关系(例如用户点击/购买的顺序)；
- 动态调整记忆单元的内容(即根据新的输入决定保留多少历史信息，丢弃多少旧信息)

##### GRU的公式机制：

- **更新门：**保留多少旧信息

  <p>
      <img src="https://hhhi21g.github.io/assets/img/SR/ar04/a0.png" alt="alt text" style="zoom:80%;" />
  </p>

- **重置门：**遗忘多少旧信息

  <p>
      <img src="https://hhhi21g.github.io/assets/img/SR/ar04/a1.png" alt="alt text" style="zoom:80%;" />
  </p>

- **候选隐藏状态：**新的候选特征

  <p>
      <img src="https://hhhi21g.github.io/assets/img/SR/ar04/a2.png" alt="alt text" style="zoom:80%;" />
  </p>

- **最终隐藏状态：**综合旧信息和新信息，得到更新后的状态

  <p>
      <img src="https://hhhi21g.github.io/assets/img/SR/ar04/a3.png" alt="alt text" style="zoom:80%;" />
  </p>

> 严格顺序更新，序列信息没有显式的全局交互；对远距离的item依赖捕捉能力不足；
>
> 导致GRU在复杂推荐场景里，难以充分建模序列中所有item的复杂关系

#### 2. Linear Attention Mechanism(线性注意力机制)

传统点积注意力计算复杂度：O(N<sup>2</sup>d)

线性注意力机制计算复杂度：O(Nd<sup>2</sup>)

<p>
    <img src="https://hhhi21g.github.io/assets/img/SR/ar04/a4.png" alt="alt text" style="zoom:80%;" />
</p>

- X<sub>1</sub>, X<sub>2</sub>: 行归一化(row-wise L2 normalization), 列归一化(column-wise L2 normalization)；
- Q,K,V: 可学习的查询(Query), 键(Key), 值(Value)矩阵；
- A‘：注意力得分。

> 注：ELU（Exponential Linear Unit, 指数线性单元）
>
> <p>
>     <img src="https://hhhi21g.github.io/assets/img/SR/ar04/a5.png" alt="alt text" style="zoom:80%;" />
> </p>

****

### GLINT-RU

#### 1. 总体结构

<p>
    <img src="https://hhhi21g.github.io/assets/img/SR/ar04/a6.png" alt="alt text" style="zoom:80%;" />
</p>

#### 2. Item Embedding Layer

For sequential recommendation tasks, **information on items interacted by users** should be **encoded to tensor** through the embedding layer.

- N: 用户-物品交互序列的长度；
- d: embedding size，嵌入向量的维度

对于一个交互序列**s<sub>i</sub>** = [v<sub>1</sub>, v<sub>2</sub>, ..., v<sub>n</sub>, ... , v<sub>n<sub>i</sub></sub>], 第n个物品**v<sub>n</sub>** ∈ R<sup>D<sub>n</sub></sup>，可通过下面公式投影为**e<sub>n</sub>**:

<p>
    <img src="https://hhhi21g.github.io/assets/img/SR/ar04/a7.png" alt="alt text" style="zoom:80%;" />
</p>

> W<sub>n</sub> ∈ R<sup>d x D<sub>n</sub></sup>, 可训练的权重矩阵

输出: **E** = [e<sub>1</sub>, e<sub>2</sub>, ... , e<sub>N</sub>]<sup>T</sup>.

传统的基于Transformer模型中，通常需要位置嵌入，因为注意力机制本身无法编码时间信息；本文中采用**GRU模块**来建模项目的时间依赖关系，因此不添加位置嵌入层。

#### 3. Dense Selective GRU

<p>
    <img src="https://hhhi21g.github.io/assets/img/SR/ar04/a8.png" alt="alt text" style="zoom:80%;" />
</p>

##### 3.1 Dense GRU module

- 输入前，做**Temporal Conv1d**:

  <p>
      <img src="https://hhhi21g.github.io/assets/img/SR/ar04/a9.png" alt="alt text" style="zoom:80%;" />
  </p>

  - X：经Item Embedding Layer获得的tensor, 形状N x d, 每一行是一个item的嵌入向量；
  - **卷积核大小：k**，在计算第n个输出时，考虑的是输入序列中相邻k个item的特征；
  - C：仍然是**长度为N**的序列(使用padding)，每个元素C<sub>n</sub>是经过卷积后，第n个时间步的局部特征表示。

- **GRU更新机制**：

  <p>
      <img src="https://hhhi21g.github.io/assets/img/SR/ar04/a10.png" alt="alt text" style="zoom:80%;" />
  </p>

  - 隐藏状态**h<sub>n</sub>**被拆分为两部分：**hat h<sub>n</sub>**: 潜在的item表示，**p<sub>n</sub>**: 细粒度的位置信息；
  - Dense GRU: 将h<sub>n</sub>拆分，显示建模内容 + 位置依赖；避免了额外的位置嵌入层；

- **Channel Crossing Layer（通道交叉层）：**

  <p>
      <img src="https://hhhi21g.github.io/assets/img/SR/ar04/a11.png" alt="alt text" style="zoom:80%;" />
  </p>

  - H：GRU的输出

- 输出后，再做**Temporal Conv1d**:

  <p>
      <img src="https://hhhi21g.github.io/assets/img/SR/ar04/a12.png" alt="alt text" style="zoom:80%;" />
  </p>

  - G：选择性门控函数；
  - Y：dense selective GRU模块生成的输出矩阵；
  - 通过两个卷积层与GRU单元相结合，使得每个隐藏状态不仅依赖于紧邻的输入时间步，还能够从更多历史行为中学习表示。		

<br>

##### 3.2 Selective Gate

<p>
    <img src="https://hhhi21g.github.io/assets/img/SR/ar04/a13.png" alt="alt text" style="zoom:80%;" />
</p>

- C: 输入序列，也即GRU的输入；
- H：GRU隐状态输出序列；
- ⊗：逐元素相乘;
- Φ(H): 对H做线性或非线性变换；
- SiLU(x) = x · σ(x).

#### 4. Expert Mixing Block

<p>
    <img src="https://hhhi21g.github.io/assets/img/SR/ar04/a14.png" alt="alt text" style="zoom:80%;" />
</p>

the two employed experts are **parallel** in the framework（Linear Attention & Dense Selective GRU）

- 通过mixing gate给两个experts分配权重：

  <p>
      <img src="https://hhhi21g.github.io/assets/img/SR/ar04/a15.png" alt="alt text" style="zoom:80%;" />
  </p>

  - α<sub>1</sub><sup>(t)</sup> , α<sub>2</sub><sup>(t)</sup> ：第t次训练迭代的可训练混合参数；

- 使用数据感知门过滤输出(data-aware gate):

  <p>
      <img src="https://hhhi21g.github.io/assets/img/SR/ar04/a16.png" alt="alt text" style="zoom:80%;" />
  </p>

  - X: expert mixing block的输入

#### 5. Gated MLP(Multilayer Perceptron, 多层感知机) Block

<p>
    <img src="https://hhhi21g.github.io/assets/img/SR/ar04/a17.png" alt="alt text" style="zoom:80%;" />
</p>

- Z: expert mixing block的输出；
- P：gated linear layer的输出；
- R：item representation;

<p>
    <img src="https://hhhi21g.github.io/assets/img/SR/ar04/a18.png" alt="alt text" style="zoom:80%;" />
</p>

- 物品表示R与物品嵌入e<sub>i</sub>做内积，得到原始得分；
- softmax对所有候选物品进行归一化，得到每个物品的预测概率；
- hat y<sub>i</sub> ：第i个物品的推荐分数。

****

### 复杂度分析

序列长度为N，嵌入维度d，卷积核大小k，整体时间复杂度为O((2k+12)Nd<sup>2</sup>)

****

### 实验

leave-one-out策略，倒数第二个交互序列用于验证

- Recall

  <p>
      <img src="https://hhhi21g.github.io/assets/img/SR/ar04/a19.png" alt="alt text" style="zoom:80%;" />
  </p>

- Mean Reciprocal Rank(MRR, 平均倒数排名)

  <p>
      <img src="https://hhhi21g.github.io/assets/img/SR/ar04/a20.png" alt="alt text" style="zoom:80%;" />
  </p>

- Normalized Discounted Cumulative Gain(NDCG)

  <p>
      <img src="https://hhhi21g.github.io/assets/img/SR/ar04/a21.png" alt="alt text" style="zoom:80%;" />
  </p>

> 均为**越大越好**

**移除Gated MLP**: 称为Light GLINT-RU, 适合资源受限场景

#### 参数影响：

- Kernel size k:
  - 增大k可以聚合更多item信息，学习更广泛的上下文，从而提升性能；
  - 过大，可能引入无关信息，导致准确率略微下降；
  - 所有kernel size下均稳定高效，增大对推理时间和GPU内存占用影响较小。
- Hidden size d:
  - 小hidden size下，也能达到其他基线的性能上限；

****