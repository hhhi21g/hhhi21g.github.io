---
layout: post

title: base02_Transformer

date: 2025-09-07 20:46:23 +0900

categories: [推荐系统]
tags: [补充]
---



### Encoder

#### 位置编码

<p >
<img src="https://hhhi21g.github.io/assets/img/SR/ar06/a25.png" alt="alt text" style="zoom:60%;" />
</p>

**函数示意图：**

<p >
<img src="https://hhhi21g.github.io/assets/img/SR/ar06/a26.png" alt="alt text" style="zoom:60%;" />
</p>

word embedding 与对应位置函数值相加，得到位置编码

****

#### Self-Attention

<p >
<img src="https://hhhi21g.github.io/assets/img/SR/ar06/a27.png" alt="alt text" style="zoom:60%;" />
</p>

- 计算Q, K, V时，对于不同token，权重相同

****

#### Residual Connection

<p >
<img src="https://hhhi21g.github.io/assets/img/SR/ar06/a28.png" alt="alt text" style="zoom:60%;" />
</p>

- 能够快速的并行训练

****

### Decoder

<p >
<img src="https://hhhi21g.github.io/assets/img/SR/ar06/a29.png" alt="alt text" style="zoom:60%;" />
</p>

- self-attention部分同Encoder；
- 计算Q，K，V时，与Encoder的权重不同；

<p >
<img src="https://hhhi21g.github.io/assets/img/SR/ar06/a30.png" alt="alt text" style="zoom:60%;" />
</p>